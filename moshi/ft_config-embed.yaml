# data
data:
  eval_data: ''
  shuffle: true
  train_data: '/users/s1f102201582/projects/mhcc-moshi/moshi/data/v2/data.jsonl'

# model
moshi_paths: 
  hf_repo_id: "nu-dialogue/j-moshi-ext"
  
full_finetuning: false # Activate lora.enable if partial finetuning
lora:
  enable: true # Set to False if full_finetuning is True
  rank: 128
  scaling: 2.
  ft_embed: true # Optional, set to True if you want to finetune the embedding layer

first_codebook_weight_multiplier: 100.
text_padding_weight: .5

# optim
duration_sec: 300
batch_size: 8
max_steps: 2000
gradient_checkpointing: true
optim:
  lr: 4e-6
  weight_decay: 0.1
  pct_start: 0.05

# other
seed: 0
log_freq: 1
eval_freq: 100
do_eval: false
do_ckpt: true
ckpt_freq: 100


# Must be False if full_finetuning is True
save_adapters: true

run_dir: "/users/s1f102201582/projects/mhcc-moshi/moshi/output/v2.1"  # Fill

# This part is optional and can be kept commented out
wandb:
  project: "mhcc-moshi-v2" # your wandb project name
  run_name: "step1000-batch1-data1093-ft_embed" # your wandb run name
  key: "0f695e81108b061ca6eef27a78cb1c805453af5a" # your wandb api key
  offline: False