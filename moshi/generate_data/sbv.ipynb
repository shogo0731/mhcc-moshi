{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276dc24c-a03a-4f02-923b-4963f2307572",
   "metadata": {},
   "source": [
    "# FT用データ生成スクリプト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "683a8852-50c8-4ba2-a611-589ebd76bc9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !conda install -y -c conda-forge kalpy \\\n",
    "# kaldi \\\n",
    "# pynini\n",
    "\n",
    "# # パッケージインストール\n",
    "# !pip install -r requirements.sbv.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a816459b-765b-42cb-a7d1-10a1211a3fa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69a5268f-a2e3-48e3-ab2a-54e9e5e298e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mfa\n",
    "# # 日本語辞書のダウンロード\n",
    "# !mfa model download dictionary japanese_mfa\n",
    "\n",
    "# # 日本語音響モデルのダウンロード\n",
    "# !mfa model download acoustic japanese_mfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e391c-3079-4a7f-8eeb-e122e07f6707",
   "metadata": {},
   "source": [
    "## テキスト対話データ生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1666ef7b-1012-47c3-9229-6082e2d04f8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "import ast\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# .envファイル読み込み\n",
    "load_dotenv(\"/users/s1f102201582/projects/mhcc-moshi/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "473c6dfd-9d4d-4bac-904a-df31dcd1a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "from os.path import join, expanduser\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "BASE_URL = \"https://api.openai.iniad.org/api/v1\"\n",
    "MODEL='gemini-2.5-flash'\n",
    "TEMPERATURE = 1.0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,4\"\n",
    "\n",
    "# 生成する音声のサンプリングレート\n",
    "setting_sr = 16000\n",
    "\n",
    "#対話音声データの個数を指定\n",
    "gen_dial_num = 5\n",
    "\n",
    "# すでに作成した対話データを削除するかどうか\n",
    "IS_REMOVE_EXIST_FILE = True\n",
    "\n",
    "# ftに使うjsonとaudioの出力フォルダパス\n",
    "home_dir = expanduser(\"~\")\n",
    "json_dir_path = join(home_dir, \"projects/mhcc-moshi/moshi/data/v1/data_stereo\")\n",
    "audio_dir_path = join(home_dir, \"projects/mhcc-moshi/moshi/data/v1/data_stereo\")\n",
    "\n",
    "# mfa関連のパス\n",
    "model_dir = join(home_dir, \"Documents/MFA/pretrained_models/acoustic/japanese_mfa.zip\")\n",
    "mfa_input_dir = join(home_dir, \"projects/mhcc-moshi/moshi/data/v1/mfa_input\")\n",
    "mfa_output_dir = join(home_dir, \"projects/mhcc-moshi/moshi/data/v1/mfa_output\")\n",
    "\n",
    "#RAGで読み取るPDFのパス\n",
    "rag_pdf_dir = join(home_dir, \"projects/mhcc-moshi/mental_docs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf8bffea-83d1-44b8-9e3d-0bd71321ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_paths = [\n",
    "    json_dir_path,\n",
    "    audio_dir_path,\n",
    "    mfa_input_dir,\n",
    "    mfa_output_dir,\n",
    "]\n",
    "\n",
    "for p in base_paths:\n",
    "    if not os.path.isdir(p):\n",
    "        os.makedirs(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bbc14b9b-7981-4975-a07b-16114cf7b3af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model定義\n",
    "model = ChatGoogleGenerativeAI(\n",
    "                 model=MODEL,\n",
    "                 temperature=TEMPERATURE)\n",
    "\n",
    "# 埋め込みモデル定義\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=BASE_URL,\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# データベース定義\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"collection\",\n",
    "    embedding_function=embeddings,\n",
    "    # persist_directory = \"/path/to/db_file\" # if necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97a271ae-62d4-44d1-b10d-8a8e38796ad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/3 [00:00<?, ?it/s]Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "    rag_pdf_dir,\n",
    "    glob=\"*.pdf\",\n",
    "    show_progress=True,\n",
    "    loader_cls=PDFMinerLoader,\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "669019e5-a634-40d8-b0f8-31abe0fb4877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Debug\n",
    "# for doc in docs:\n",
    "#     print(\"-------------------------------------------------\")\n",
    "#     print(doc.metadata)\n",
    "#     print(len(doc.page_content))\n",
    "#     print(doc.page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ff84496-9aa7-45d9-89d3-3b8cde22fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#読み込んだ文章データをオーバーラップ200文字で1000文字づつ分割\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True, # 分割前の文章のインデックスを追跡\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# データベースにデータを追加\n",
    "document_ids = vector_store.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "757916d2-87c6-4033-8a23-02f0eaa6b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query, k=2)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d58e3775-2aad-4363-b893-f7d74f62d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Dialogue(BaseModel):\n",
    "    \"\"\"対話データを構成する対話クラス\"\"\"\n",
    "    speaker: Literal[\"A\", \"B\"] = Field(..., description=\"話者。Aはカウンセラー、Bはクライエントを表す。\")\n",
    "    text: str = Field(..., description=\"話者が話した内容。\")\n",
    "\n",
    "class Dialogues(BaseModel):\n",
    "    \"\"\"カウンセリングを目的としたカウンセリング対話データ\"\"\"\n",
    "    dialogues: list[Dialogue] = Field(..., description=\"対話データを構成する対話クラスのリスト。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c850f79b-c9ac-4722-b165-481de934e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model, \n",
    "    tools=[],\n",
    "    middleware=[prompt_with_context],\n",
    "    response_format=ToolStrategy(\n",
    "        Dialogues,\n",
    "        handle_errors=\"フォーマットに合うように、もう一度対話データを生成してください。\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "261f3908-86f7-4ddb-9565-c7b56e25632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#promptを作成\n",
    "import random\n",
    "\n",
    "\n",
    "sessions = [\n",
    "    \"【段階：初期】信頼関係を築きつつ、悩みの背景を深掘りするシーン\",\n",
    "    \"【段階：中期】クライエントの「すべき思考」に焦点を当て、認知の歪みを扱うシーン\",\n",
    "    \"【段階：終結期】これまでのセッションを振り返り、終結に向けて準備するシーン\",\n",
    "]\n",
    "\n",
    "def gen_prompt_txt():\n",
    "    choiced = random.randint(0, 2)\n",
    "    choiced_session = sessions[choiced]\n",
    "    prompt_txt = f\"\"\"メンタルヘルスケアカウンセリングのセッションをシミュレーションしてください。\n",
    "シミュレーションしたい「段階」と「テーマ」:\n",
    "{choiced_session}\n",
    "\n",
    "役割定義:\n",
    "A (カウンセラー): メンタルヘルスケアの専門知識を持つ経験豊富なカウンセラー。傾聴と共感の姿勢を基本とし、クライエントの言葉を促すように、優しく、自然な話し言葉（「〜ですね」「〜でしたか」など）を使います。\n",
    "B (クライエント): 仕事上の悩みだけでなく、日常生活全般に対して漠然とした不安や焦りを感じている人物。\n",
    "\n",
    "対話の要件:\n",
    "スタイル: 実際の会話の文字起こしのように、堅苦しくない自然な「話し言葉」を使用してください。\n",
    "相槌 (あいづち): カウンセラー（A）は、クライエント（B）の話を促し、共感を示すため、「ええ」「はい」「そうなんですね」「なるほど」といった細かな相槌を頻繁に、適切なタイミングで挿入してください。\n",
    "構成: 会話が途中で途切れるのではなく、初回のヒアリングとして「一区切り」がつき、自然に終了する流れにしてください（例：次回の約束、今回のまとめなど）。\n",
    "分量: 会話の往復は合計12〜20ターン程度、全体の文字数が合計500〜800文字程度になるように構成してください。\n",
    "\"\"\"\n",
    "    return prompt_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d76aa80-c527-41a1-963a-5e60dc2307e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "\n",
    "max_retries = 5\n",
    "base_wait_time = 1 # minutes\n",
    "\n",
    "# テキスト対話生成関数\n",
    "def gen_txt_dialogue():\n",
    "    prompt = gen_prompt_txt()\n",
    "\n",
    "    # レート制限に引っかかることがあるため、例外処理\n",
    "    for i in range(1, max_retries+1):\n",
    "        try:\n",
    "            resp = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": prompt}]})\n",
    "            break\n",
    "        except ResourceExhausted as e:\n",
    "            if i < max_retries - 1:\n",
    "                wait_time = (base_wait_time ** i) * 60\n",
    "                time.sleep(wait_time)\n",
    "            # max_retries回失敗した場合はエラーを起こす\n",
    "            else:\n",
    "                raise e\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    dialogues_list = resp[\"structured_response\"].dialogues\n",
    "    return dialogues_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5813f865-a27c-473c-8acc-e8739dd606ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DEBUG\n",
    "# txt_dialogue = gen_txt_dialogue()\n",
    "# print(txt_dialogue)\n",
    "# lst_dialogue = txt_to_lst(txt_dialogue)\n",
    "# print(lst_dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd4da5-0ba6-4958-929d-c547e7c1f72c",
   "metadata": {},
   "source": [
    "## テキスト対話データを音声対話データに変換 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2caf61a3-b36a-48a1-8604-b8d720c5265d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jvnv-F2-jp/jvnv-F2_e166_s20000.safetensors\n",
      "jvnv-F2-jp/config.json\n",
      "jvnv-F2-jp/style_vectors.npy\n",
      "jvnv-M2-jp/jvnv-M2-jp_e159_s17000.safetensors\n",
      "jvnv-M2-jp/config.json\n",
      "jvnv-M2-jp/style_vectors.npy\n"
     ]
    }
   ],
   "source": [
    "from style_bert_vits2.nlp import bert_models\n",
    "from style_bert_vits2.constants import Languages\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "from style_bert_vits2.tts_model import TTSModel\n",
    "\n",
    "bert_models.load_model(Languages.JP, \"ku-nlp/deberta-v2-large-japanese-char-wwm\")\n",
    "bert_models.load_tokenizer(Languages.JP, \"ku-nlp/deberta-v2-large-japanese-char-wwm\")\n",
    "assets_root = Path(\"model_assets\")\n",
    "\n",
    "# # 子春音あみ\n",
    "# model_file = \"koharune-ami/koharune-ami.safetensors\"\n",
    "# config_file = \"koharune-ami/config.json\"\n",
    "# style_file = \"koharune-ami/style_vectors.npy\"\n",
    "# hf_repo = \"litagin/sbv2_koharune_ami\"\n",
    "\n",
    "# # あみたろ\n",
    "# model_file = \"amitaro/amitaro.safetensors\"\n",
    "# config_file = \"amitaro/config.json\"\n",
    "# style_file = \"amitaro/style_vectors.npy\"\n",
    "# hf_repo = \"litagin/sbv2_amitaro\"\n",
    "\n",
    "\n",
    "# デフォルトの女性2\n",
    "model_file = \"jvnv-F2-jp/jvnv-F2_e166_s20000.safetensors\"\n",
    "config_file = \"jvnv-F2-jp/config.json\"\n",
    "style_file = \"jvnv-F2-jp/style_vectors.npy\"\n",
    "hf_repo = \"litagin/style_bert_vits2_jvnv\"\n",
    "\n",
    "for file in [model_file, config_file, style_file]:\n",
    "    print(file)\n",
    "    hf_hub_download(hf_repo, file, local_dir=\"model_assets\")\n",
    "\n",
    "A_model = TTSModel(\n",
    "    model_path=assets_root / model_file,\n",
    "    config_path=assets_root / config_file,\n",
    "    style_vec_path=assets_root / style_file,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# デフォルトの男性2\n",
    "model_file = \"jvnv-M2-jp/jvnv-M2-jp_e159_s17000.safetensors\"\n",
    "config_file = \"jvnv-M2-jp/config.json\"\n",
    "style_file = \"jvnv-M2-jp/style_vectors.npy\"\n",
    "\n",
    "for file in [model_file, config_file, style_file]:\n",
    "    print(file)\n",
    "    hf_hub_download(hf_repo, file, local_dir=\"model_assets\")\n",
    "\n",
    "B_model = TTSModel(\n",
    "    model_path=assets_root / model_file,\n",
    "    config_path=assets_root / config_file,\n",
    "    style_vec_path=assets_root / style_file,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c975667-8174-4762-9db7-e8a602d2adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_audio_synth_prompt(text_dialogue_list):\n",
    "    resp = \"\"\n",
    "    resp_header =  \"\"\"あなたがこれから音声合成するテキストは以下の対話内容のワンフレーズです。\n",
    "この対話の文脈に合うように音声合成してください。\n",
    "\n",
    "<対話内容の全文>\"\"\"\n",
    "    resp += resp_header\n",
    "    for text_dial in text_dialogue_list:\n",
    "        resp += f\"\\n{text_dial.speaker}: {text_dial.text}\"\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "568879f5-82d9-488f-a281-d3ad816f248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def sbv_tts(text: str, speaker: Literal[\"A\", \"B\"], assist_text=None):\n",
    "    if speaker == \"A\":\n",
    "        sr, audio = A_model.infer(\n",
    "            text = text,\n",
    "            style='Happy',\n",
    "            style_weight=1,\n",
    "            split_interval = 0.3,\n",
    "            use_assist_text = True if assist_text is not None else None,\n",
    "            assist_text = assist_text\n",
    "        )\n",
    "    else:\n",
    "        sr, audio = B_model.infer(\n",
    "            text = text,\n",
    "            style='Sad',\n",
    "            style_weight=1,\n",
    "            split_interval = 0.3,\n",
    "            use_assist_text = True if assist_text is not None else None,\n",
    "            assist_text = assist_text\n",
    "        )\n",
    "    \n",
    "    return sr, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9df1471-cccf-40fb-bb30-23ba6fdeba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def gen_audio_dialogue(text_dialogue_list, prompt):\n",
    "    # 音声ファイルを順番に生成（ファイルは不要なのでwave配列で持つ）\n",
    "    wav_data = []\n",
    "    for dial in text_dialogue_list:\n",
    "        speaker = dial.speaker\n",
    "        sr, wav = sbv_tts(dial.text, speaker, prompt)\n",
    "\n",
    "        # サンプリングレートを変換\n",
    "        if sr != setting_sr:\n",
    "            # 16ビット整数のデータを、-1.0から1.0の範囲に収まる浮動小数点数に正規化\n",
    "            wav = wav.astype(np.float32) / 32768.0\n",
    "            wav = librosa.resample(wav, orig_sr=sr, target_sr=setting_sr)\n",
    "\n",
    "        # 0.3秒間の無音時間を追加\n",
    "        duration_sec = 0.3\n",
    "        num_silent_samples = int(setting_sr*duration_sec)\n",
    "        silence = np.zeros(num_silent_samples, dtype=wav.dtype)\n",
    "        wav_with_silence = np.concatenate((wav, silence))\n",
    "        wav_data.append(wav_with_silence)\n",
    "    \n",
    "    # 最終的な音声長を決定\n",
    "    max_len = sum([len(w) for w in wav_data])\n",
    "    \n",
    "    # ステレオ音声用（2チャンネル×最大長）の空配列をゼロ初期化で作成\n",
    "    stereo = np.zeros((2, max_len), dtype=np.float32)\n",
    "    \n",
    "    pos = 0\n",
    "    for i, wav in enumerate(wav_data):\n",
    "        ch = i%2  # 0:左(A), 1:右(B)\n",
    "        stereo[ch, pos:pos+len(wav)] += wav\n",
    "        pos += len(wav)\n",
    "    \n",
    "    # 転置(-1,2)する\n",
    "    stereo = stereo.T\n",
    "    return stereo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa95302-64ba-4175-ab42-4939b02ee1be",
   "metadata": {},
   "source": [
    "## mfa(montreal force alignment)による音声アラインメント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b201df8-c906-4cf7-ae5b-d94a7aa1e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def correct_json(full_text, align_json):\n",
    "    new_align_json = copy.deepcopy(align_json)\n",
    "    segments = new_align_json[\"tiers\"][\"words\"][\"entries\"]\n",
    "    checked_len = 0\n",
    "    prev_checked_len = 0\n",
    "    i = 0\n",
    "    while i < len(segments):\n",
    "        if re.search(f\"^<unk>|<sil>$\", segments[i][2]):\n",
    "            if i == 0:\n",
    "                if re.search(f\"^<unk>|<sil>$\", segments[i+1][2]):\n",
    "                    end_time = 0\n",
    "                    while re.search(f\"^<unk>|<sil>$\", segments[i+1][2]):\n",
    "                        end_time = segments[i+1][1]\n",
    "                        segments.pop(i+1)\n",
    "                    segments[i][1] = end_time\n",
    "                \n",
    "                m = re.search(f\"^(.+?){segments[i+1][2]}\", full_text[checked_len:])\n",
    "                match_text = m.groups()\n",
    "                segments[i][2] = match_text[0]\n",
    "            elif i == len(segments)-1:\n",
    "                m = re.search(f\"{segments[i-1][2]}(.+?)$\", full_text[checked_len:])\n",
    "                match_text = m.groups()\n",
    "                segments[i][2] = match_text[0]\n",
    "            else:\n",
    "                if re.search(f\"^<unk>|<sil>$\", segments[i+1][2]):\n",
    "                    end_time = 0\n",
    "                    while re.search(f\"^<unk>|<sil>$\", segments[i+1][2]):\n",
    "                        end_time = segments[i+1][1]\n",
    "                        segments.pop(i+1)\n",
    "                    segments[i][1] = end_time\n",
    "                m = re.search(f\"^{segments[i-1][2]}(.+?){segments[i+1][2]}\", full_text[prev_checked_len:])\n",
    "                match_text = m.groups()\n",
    "                segments[i][2] = match_text[0]\n",
    "        else:\n",
    "            if re.search(f\"^([。、,.!?！？…「」]){segments[i][2]}.*$\", full_text[checked_len:]):\n",
    "                m = re.search(f\"^([。、,.!?！？…「」]){segments[i][2]}.*$\", full_text[checked_len:])\n",
    "                match_punc = m.groups()\n",
    "                segments[i][2] = match_punc[0] + segments[i][2]\n",
    "            elif re.search(f\"^{segments[i][2]}([。、,.!?！？…「」]).*$\", full_text[checked_len:]):\n",
    "                m = re.search(f\"^{segments[i][2]}([。、,.!?！？…「」]).*$\", full_text[checked_len:])\n",
    "                match_punc = m.groups()\n",
    "                segments[i][2] = segments[i][2] + match_punc[0]\n",
    "                \n",
    "        prev_checked_len = checked_len\n",
    "        checked_len += len(segments[i][2])\n",
    "        i += 1\n",
    "    return new_align_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09aaef40-fca0-46e3-96b2-0c04a1ff984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, expanduser\n",
    "import subprocess\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "def alignment_channel(channel, target_dir_name):\n",
    "    input_dir_path = join(mfa_input_dir, target_dir_name)\n",
    "    output_dir_path = join(mfa_output_dir, target_dir_name)\n",
    "    os.makedirs(input_dir_path, exist_ok=True)\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "    subprocess.run([\n",
    "        \"mfa\",\n",
    "        \"align\",\n",
    "        input_dir_path,\n",
    "        \"japanese_mfa\",\n",
    "        model_dir,\n",
    "        output_dir_path,\n",
    "        \"--quiet\",\n",
    "        \"--overwrite\",\n",
    "        \"--clean\",\n",
    "        \"--final_clean\",\n",
    "        \"--output_format\", \"json\",\n",
    "        \"--beam\", \"1000\",\n",
    "        \"--retry_beam\", \"4000\",\n",
    "    ])      \n",
    "\n",
    "def parse_ft_json(json_data):\n",
    "    result = {\"alignments\": []}\n",
    "\n",
    "    segments = json_data[\"tiers\"][\"words\"][\"entries\"]\n",
    "    for segment in segments:\n",
    "        result[\"alignments\"].append([\n",
    "            segment[2],\n",
    "            [segment[0], segment[1]],\n",
    "            \"SPEAKER_MAIN\"\n",
    "        ])\n",
    "    result[\"alignments\"].sort(key=lambda x: x[1][0])\n",
    "    return result\n",
    "\n",
    "def alignment_audio_dialogue(text_dialogue_list, audio_path, idx):\n",
    "    json_list = []\n",
    "    audio, sr = sf.read(audio_path)\n",
    "    \n",
    "    result = \"\"\n",
    "    target_dir_name = str(idx)\n",
    "    target_dir = os.path.join(mfa_input_dir, target_dir_name)\n",
    "    if not os.path.isdir(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    target_text_file = os.path.join(target_dir, f\"{idx}.txt\")\n",
    "\n",
    "    oneline_text = \"\"\n",
    "    for dial in text_dialogue_list:\n",
    "        result += dial.text + \"\\n\"\n",
    "        oneline_text += dial.text\n",
    "    with open(target_text_file, \"w\") as f:\n",
    "        f.write(result)\n",
    "\n",
    "    wav_name = f\"{idx}.wav\"\n",
    "    src_wav_path = os.path.join(audio_dir_path, wav_name)\n",
    "    dist_wav_path = os.path.join(target_dir, wav_name)\n",
    "    shutil.copy(src_wav_path, dist_wav_path)\n",
    "\n",
    "    alignment_channel(audio, target_dir_name)\n",
    "    json_path = os.path.join(mfa_output_dir, target_dir_name, f\"{idx}.json\")\n",
    "    json_data = \"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    try:\n",
    "        correct_json_data = correct_json(oneline_text, json_data)\n",
    "        ft_json = parse_ft_json(correct_json_data)\n",
    "    except:\n",
    "        print(f\"jsonファイル {idx}.json の訂正に失敗しました。\")\n",
    "        ft_json= parse_ft_json(json_data)\n",
    "    \n",
    "    return ft_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b917fe-46ac-43a6-a5e7-c9903095616a",
   "metadata": {},
   "source": [
    "## フォルダ初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17a8d44f-adba-483e-9d5d-65dae116d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_file_name():\n",
    "    wav_file_pattern = r\"^(\\d+)\\.wav$\"\n",
    "    num = -1\n",
    "    for file in os.listdir(audio_dir_path):\n",
    "        print(file)\n",
    "        if not os.path.exists(os.path.join(audio_dir_path, file)):\n",
    "            continue\n",
    "        if not re.match(wav_file_pattern, file):\n",
    "            continue\n",
    "\n",
    "        match_obj = re.match(wav_file_pattern, file)\n",
    "        get_number = int(match_obj.groups()[0])\n",
    "        print(get_number)\n",
    "        if num < get_number:\n",
    "            num = get_number\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4895d7b4-d9e6-43c3-b4b6-d21b0d4eac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import shutil\n",
    "\n",
    "def delete_files(dir_path):\n",
    "    shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "if IS_REMOVE_EXIST_FILE:\n",
    "    file_name_num = -1\n",
    "    for dir_path in base_paths:\n",
    "        delete_files(dir_path)\n",
    "else:\n",
    "    file_name_num = get_file_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535afd9-ad91-43af-a583-32cf4525864e",
   "metadata": {},
   "source": [
    "## メイン処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92baf05a-d14e-4a4b-8f3d-98bdc5043914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m11-04 23:49:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、今日はどんなことでお話しに来られましたか？\n",
      "\u001b[32m11-04 23:49:05\u001b[0m |\u001b[1m  INFO  \u001b[0m| infer.py:24 | Using JP-Extra model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs1/s1f102201582/anaconda3/envs/mfa_unit/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m11-04 23:49:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| safetensors.py:50 | Loaded 'model_assets/jvnv-F2-jp/jvnv-F2_e166_s20000.safetensors' (iteration 166)\n",
      "\u001b[32m11-04 23:49:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:49:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは。えっと、最近、仕事のことがずっと頭から離れなくて、なんだかいつも焦っているような、漠然とした不安があるんです。特に何が、というわけでもないんですけど…。\n",
      "\u001b[32m11-04 23:49:06\u001b[0m |\u001b[1m  INFO  \u001b[0m| infer.py:24 | Using JP-Extra model\n",
      "\u001b[32m11-04 23:49:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| safetensors.py:50 | Loaded 'model_assets/jvnv-M2-jp/jvnv-M2-jp_e159_s17000.safetensors' (iteration 159)\n",
      "\u001b[32m11-04 23:49:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:49:07\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですか。お仕事のことで、常に焦りや不安を感じていらっしゃるんですね。はい。それは、いつ頃から感じ始めたことでしたか？\n",
      "\u001b[32m11-04 23:49:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:49:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はっきりとは覚えてないんですが、半年くらい前からでしょうか。部署が変わってから、新しい業務が増えて、ずっと落ち着かない感じが続いています。家に帰っても、仕事のことを考えてしまって…。\n",
      "\u001b[32m11-04 23:49:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:49:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど、部署の異動がきっかけだったのですね。ええ。家に帰られても、気持ちが休まらないというのは、とてもお辛いですね。そうなんですね。具体的に、どんな時に一番そう感じますか？\n",
      "\u001b[32m11-04 23:49:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:49:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうですね…朝、目が覚めた瞬間から「あ、また一日が始まる」って、ちょっと憂鬱な気持ちになってしまいます。休日も、何かしないと、って焦るばかりで、結局何も手につかない、なんてこともよくあって。\n",
      "\u001b[32m11-04 23:49:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:49:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "朝起きるのが辛かったり、休日もリラックスできなかったり…。常に何かを追い立てられているような感覚があるのですね。はい。それは大変な状況ですね。\n",
      "\u001b[32m11-04 23:49:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:49:08\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、本当に。このままだとどうにかなってしまいそうで、自分でもどうしたらいいのか分からなくて、今日ここに来てみました。\n",
      "\u001b[32m11-04 23:49:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:49:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ、よく来てくださいましたね。そうなんですね。漠然とした不安や焦り、そしてそれが日常生活にも影響しているということ、お話しくださってありがとうございます。今日はまず、今感じていらっしゃることをお伺いできて、よかったです。\n",
      "\u001b[32m11-04 23:49:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:49:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございます。少し話せただけでも、楽になった気がします。\n",
      "\u001b[32m11-04 23:49:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:49:09\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そう言っていただけて、私も嬉しいです。今日お話しいただいたことを踏まえて、次回はもう少し具体的に、この漠然とした不安の背景や、日常生活での困りごとについて、詳しくお伺いできたらと思うのですが、いかがでしょうか？\n",
      "\u001b[32m11-04 23:49:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:49:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひ。よろしくお願いします。\n",
      "\u001b[32m11-04 23:49:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:49:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "承知いたしました。では、次回の予約について、この後ご案内させていただきますね。今日はありがとうございました。\n",
      "\u001b[32m11-04 23:49:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:49:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ありがとうございました。\n",
      "\u001b[32m11-04 23:49:10\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/projects/mhcc-moshi/moshi/data/v1/mfa_output/\u001b[0m\u001b[95m0...\u001b[0m \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/projects/mhcc-moshi/moshi/data/v1/mfa_output/\u001b[0m\u001b[95m0\u001b[0m!   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m185.150\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m11-04 23:53:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日はどんなことをお話ししましょうか？\n",
      "\u001b[32m11-04 23:53:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、先生。最近、色々と「こうするべきだ」って考えてしまって、すごく疲れるんです。\n",
      "\u001b[32m11-04 23:53:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:29\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "「こうするべきだ」ですか。ええ。具体的にどんな時にそう感じますか？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_insert_pause() in jpcommon_label.c: First mora should not be short pause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m11-04 23:53:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "仕事で新しいプロジェクトを任された時も、「完璧にこなすべきだ」って思って、夜遅くまで資料を作ったりしてしまうんです。\n",
      "\u001b[32m11-04 23:53:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "完璧にこなすべき、と。はい。そうなんですね。そう思うと、どんな気持ちになりますか？\n",
      "\u001b[32m11-04 23:53:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "プレッシャーがすごくて、失敗したらどうしようって不安で、全然リラックスできないんです。\n",
      "\u001b[32m11-04 23:53:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:30\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "不安になる、と。なるほど。その「完璧にこなすべきだ」という考えが、Bさんにとってどんな影響を与えていると思いますか？\n",
      "\u001b[32m11-04 23:53:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "いつも焦っているようで、楽しめないというか。周りの人はもっとうまくやっているのに、自分だけ、って。\n",
      "\u001b[32m11-04 23:53:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "周りはもっとうまくやっているのに、自分だけ、と感じるんですね。そうでしたか。そういった考えがあることに、今、気づかれたわけですね。\n",
      "\u001b[32m11-04 23:53:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、そうですね。言われてみれば、いつもそう考えている気がします。\n",
      "\u001b[32m11-04 23:53:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:31\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "今日は、その「こうするべきだ」という考えが、Bさんにどんな影響を与えているか、一緒に見ることができました。次回は、この考え方について、もう少し深く探っていきましょうか。\n",
      "\u001b[32m11-04 23:53:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、ぜひお願いします。少し、気持ちが楽になった気がします。\n",
      "\u001b[32m11-04 23:53:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それは良かったです。また来週、同じ時間で大丈夫でしょうか？\n",
      "\u001b[32m11-04 23:53:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、大丈夫です。ありがとうございます。\n",
      "\u001b[32m11-04 23:53:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-04 23:53:32\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こちらこそ。お気をつけてお帰りくださいね。\n",
      "\u001b[32m11-04 23:53:33\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/projects/mhcc-moshi/moshi/data/v1/mfa_output/\u001b[0m\u001b[95m1...\u001b[0m \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/projects/mhcc-moshi/moshi/data/v1/mfa_output/\u001b[0m\u001b[95m1\u001b[0m!   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m154.820\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 3.02 s, total: 1min 16s\n",
      "Wall time: 8min 55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import soundfile as sf\n",
    "import json\n",
    "\n",
    "for i in range(file_name_num+1, gen_dial_num+file_name_num+1):\n",
    "\n",
    "    # テキスト生成\n",
    "    txt_dialogue_list = gen_txt_dialogue()\n",
    "\n",
    "    # 音声合成のためのプロンプト生成\n",
    "    audio_synth_prompt = build_audio_synth_prompt(txt_dialogue_list)\n",
    "\n",
    "    # 対話テキストを音声合成\n",
    "    stereo = gen_audio_dialogue(txt_dialogue_list, audio_synth_prompt)\n",
    "    \n",
    "    wav_name = f\"{i}.wav\"\n",
    "    audio_file_path = os.path.join(audio_dir_path, wav_name)\n",
    "\n",
    "    # wavファイル出力\n",
    "    sf.write(audio_file_path, stereo, setting_sr)\n",
    "\n",
    "    # 音声アラインメント\n",
    "    json_data = alignment_audio_dialogue(txt_dialogue_list, audio_file_path, i)\n",
    "\n",
    "    json_name = f\"{i}.json\"\n",
    "    json_file_path = os.path.join(json_dir_path, json_name)\n",
    "    \n",
    "    # JSON出力\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241b1bb-1f89-4f17-bafa-4eddb96ed3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
