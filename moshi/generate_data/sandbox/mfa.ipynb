{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb76be09-a929-4e09-947b-5f656541f094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.sbv.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d185bc-1fe0-4ead-95de-0ef4b34a321d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !conda install -y -c conda-forge kaldi kalpy pynini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc7d51f-1d66-4e58-90bc-5d37bb3ff55e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93cc73ba-ab99-42a1-aa61-407b4f424c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 日本語辞書のダウンロード\n",
    "# !mfa model download dictionary japanese_mfa\n",
    "\n",
    "# # 日本語音響モデルのダウンロード\n",
    "# !mfa model download acoustic japanese_mfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcfe728e-9d27-430d-bb12-d50ea9323a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs1/s1f102201582/anaconda3/envs/mfa_py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "import ast\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# .envファイル読み込み\n",
    "load_dotenv(\"/users/s1f102201582/projects/mhcc-moshi/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d8dcb3e-0542-4d5a-b33e-0b695ee65921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "from os.path import join, expanduser\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "BASE_URL = \"https://api.openai.iniad.org/api/v1\"\n",
    "MODEL='gemini-2.5-flash'\n",
    "TEMPERATURE = 1.0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,4\"\n",
    "\n",
    "# 生成する音声のサンプリングレート\n",
    "setting_sr = 16000\n",
    "\n",
    "#対話音声データの個数を指定\n",
    "gen_dial_num = 1\n",
    "\n",
    "# すでに作成した対話データを削除するかどうか\n",
    "IS_REMOVE_EXIST_FILE = True\n",
    "\n",
    "# mfa関連のパス\n",
    "model_dir = \"/users/s1f102201582/Documents/MFA/pretrained_models/acoustic/japanese_mfa.zip\"\n",
    "\n",
    "#RAGで読み取るPDFのパス\n",
    "rag_pdf_dir = \"/users/s1f102201582/projects/mhcc-moshi/mental_docs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f8b381-48c5-4a9c-b21f-698ab1e44db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model定義\n",
    "model = ChatGoogleGenerativeAI(\n",
    "                 model=MODEL,\n",
    "                 temperature=TEMPERATURE)\n",
    "\n",
    "# 埋め込みモデル定義\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=BASE_URL,\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# データベース定義\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"collection\",\n",
    "    embedding_function=embeddings,\n",
    "    # persist_directory = \"/path/to/db_file\" # if necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65198716-bf01-495c-96d4-e4a4bc90c910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/3 [00:00<?, ?it/s]Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "    rag_pdf_dir,\n",
    "    glob=\"*.pdf\",\n",
    "    show_progress=True,\n",
    "    loader_cls=PDFMinerLoader,\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2182b28e-6abf-4e82-aa7f-39c05863707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#読み込んだ文章データをオーバーラップ200文字で1000文字づつ分割\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True, # 分割前の文章のインデックスを追跡\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# データベースにデータを追加\n",
    "document_ids = vector_store.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0688455-4ffb-4d02-b3a0-96d6dd4e515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query, k=2)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dec5c7ec-de1a-49fd-9688-11c444af8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Dialogue(BaseModel):\n",
    "    \"\"\"対話データを構成する対話クラス\"\"\"\n",
    "    speaker: Literal[\"A\", \"B\"] = Field(..., description=\"話者。Aはカウンセラー、Bはクライエントを表す。\")\n",
    "    text: str = Field(..., description=\"話者が話した内容。\")\n",
    "\n",
    "class Dialogues(BaseModel):\n",
    "    \"\"\"カウンセリングを目的としたカウンセリング対話データ\"\"\"\n",
    "    dialogues: list[Dialogue] = Field(..., description=\"対話データを構成する対話クラスのリスト。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8455dba0-2298-4faa-801a-c56524ad71df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model, \n",
    "    tools=[],\n",
    "    middleware=[prompt_with_context],\n",
    "    response_format=ToolStrategy(\n",
    "        Dialogues,\n",
    "        handle_errors=\"フォーマットに合うように、もう一度対話データを生成してください。\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eeaf7a6-d33d-40da-a3b4-b00a83434ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#promptを作成\n",
    "import random\n",
    "\n",
    "\n",
    "sessions = [\n",
    "    \"【段階：初期】信頼関係を築きつつ、悩みの背景を深掘りするシーン\",\n",
    "    \"【段階：中期】クライエントの「すべき思考」に焦点を当て、認知の歪みを扱うシーン\",\n",
    "    \"【段階：終結期】これまでのセッションを振り返り、終結に向けて準備するシーン\",\n",
    "]\n",
    "\n",
    "def gen_prompt_txt():\n",
    "    choiced = random.randint(0, 2)\n",
    "    choiced_session = sessions[choiced]\n",
    "    prompt_txt = f\"\"\"メンタルヘルスケアカウンセリングのセッションをシミュレーションしてください。\n",
    "シミュレーションしたい「段階」と「テーマ」:\n",
    "{choiced_session}\n",
    "\n",
    "役割定義:\n",
    "A (カウンセラー): メンタルヘルスケアの専門知識を持つ経験豊富なカウンセラー。傾聴と共感の姿勢を基本とし、クライエントの言葉を促すように、優しく、自然な話し言葉（「〜ですね」「〜でしたか」など）を使います。\n",
    "B (クライエント): 仕事上の悩みだけでなく、日常生活全般に対して漠然とした不安や焦りを感じている人物。\n",
    "\n",
    "対話の要件:\n",
    "スタイル: 実際の会話の文字起こしのように、堅苦しくない自然な「話し言葉」を使用してください。\n",
    "相槌 (あいづち): カウンセラー（A）は、クライエント（B）の話を促し、共感を示すため、「ええ」「はい」「そうなんですね」「なるほど」といった細かな相槌を頻繁に、適切なタイミングで挿入してください。\n",
    "構成: 会話が途中で途切れるのではなく、初回のヒアリングとして「一区切り」がつき、自然に終了する流れにしてください（例：次回の約束、今回のまとめなど）。\n",
    "分量: 会話の往復は合計12〜20ターン程度、全体の文字数が合計500〜800文字程度になるように構成してください。\n",
    "\"\"\"\n",
    "    return prompt_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d361867b-5179-40b7-b017-de033e255748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキスト対話生成関数\n",
    "def gen_txt_dialogue():\n",
    "    prompt = gen_prompt_txt()\n",
    "    resp = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": prompt}]})\n",
    "    dialogues_list = resp[\"structured_response\"].dialogues\n",
    "    return dialogues_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "936ac5b8-a708-47d2-8931-051e48fafab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m11-05 23:34:12\u001b[0m |\u001b[1m  INFO  \u001b[0m| bert_models.py:92 | Loaded the Languages.JP BERT model from ku-nlp/deberta-v2-large-japanese-char-wwm\n",
      "\u001b[32m11-05 23:34:13\u001b[0m |\u001b[1m  INFO  \u001b[0m| bert_models.py:154 | Loaded the Languages.JP BERT tokenizer from ku-nlp/deberta-v2-large-japanese-char-wwm\n",
      "jvnv-F2-jp/jvnv-F2_e166_s20000.safetensors\n",
      "jvnv-F2-jp/config.json\n",
      "jvnv-F2-jp/style_vectors.npy\n",
      "jvnv-M2-jp/jvnv-M2-jp_e159_s17000.safetensors\n",
      "jvnv-M2-jp/config.json\n",
      "jvnv-M2-jp/style_vectors.npy\n"
     ]
    }
   ],
   "source": [
    "from style_bert_vits2.nlp import bert_models\n",
    "from style_bert_vits2.constants import Languages\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "from style_bert_vits2.tts_model import TTSModel\n",
    "\n",
    "bert_models.load_model(Languages.JP, \"ku-nlp/deberta-v2-large-japanese-char-wwm\")\n",
    "bert_models.load_tokenizer(Languages.JP, \"ku-nlp/deberta-v2-large-japanese-char-wwm\")\n",
    "assets_root = Path(\"model_assets\")\n",
    "\n",
    "# # 子春音あみ\n",
    "# model_file = \"koharune-ami/koharune-ami.safetensors\"\n",
    "# config_file = \"koharune-ami/config.json\"\n",
    "# style_file = \"koharune-ami/style_vectors.npy\"\n",
    "# hf_repo = \"litagin/sbv2_koharune_ami\"\n",
    "\n",
    "# # あみたろ\n",
    "# model_file = \"amitaro/amitaro.safetensors\"\n",
    "# config_file = \"amitaro/config.json\"\n",
    "# style_file = \"amitaro/style_vectors.npy\"\n",
    "# hf_repo = \"litagin/sbv2_amitaro\"\n",
    "\n",
    "\n",
    "# デフォルトの女性2\n",
    "model_file = \"jvnv-F2-jp/jvnv-F2_e166_s20000.safetensors\"\n",
    "config_file = \"jvnv-F2-jp/config.json\"\n",
    "style_file = \"jvnv-F2-jp/style_vectors.npy\"\n",
    "hf_repo = \"litagin/style_bert_vits2_jvnv\"\n",
    "\n",
    "for file in [model_file, config_file, style_file]:\n",
    "    print(file)\n",
    "    hf_hub_download(hf_repo, file, local_dir=\"model_assets\")\n",
    "\n",
    "A_model = TTSModel(\n",
    "    model_path=assets_root / model_file,\n",
    "    config_path=assets_root / config_file,\n",
    "    style_vec_path=assets_root / style_file,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# デフォルトの男性2\n",
    "model_file = \"jvnv-M2-jp/jvnv-M2-jp_e159_s17000.safetensors\"\n",
    "config_file = \"jvnv-M2-jp/config.json\"\n",
    "style_file = \"jvnv-M2-jp/style_vectors.npy\"\n",
    "\n",
    "for file in [model_file, config_file, style_file]:\n",
    "    print(file)\n",
    "    hf_hub_download(hf_repo, file, local_dir=\"model_assets\")\n",
    "\n",
    "B_model = TTSModel(\n",
    "    model_path=assets_root / model_file,\n",
    "    config_path=assets_root / config_file,\n",
    "    style_vec_path=assets_root / style_file,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84543627-007c-42da-b320-5c5b06f7931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_audio_synth_prompt(text_dialogue_list):\n",
    "    resp = \"\"\n",
    "    resp_header =  \"\"\"あなたがこれから音声合成するテキストは以下の対話内容のワンフレーズです。\n",
    "この対話の文脈に合うように音声合成してください。\n",
    "\n",
    "<対話内容の全文>\"\"\"\n",
    "    resp += resp_header\n",
    "    for text_dial in text_dialogue_list:\n",
    "        resp += f\"\\n{text_dial.speaker}: {text_dial.text}\"\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5255a54c-a3ba-4586-96b2-03477928a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def sbv_tts(text: str, speaker: Literal[\"A\", \"B\"], assist_text=None):\n",
    "    if speaker == \"A\":\n",
    "        sr, audio = A_model.infer(\n",
    "            text = text,\n",
    "            style='Happy',\n",
    "            style_weight=1,\n",
    "            split_interval = 0.3,\n",
    "            use_assist_text = True if assist_text is not None else None,\n",
    "            assist_text = assist_text\n",
    "        )\n",
    "    else:\n",
    "        sr, audio = B_model.infer(\n",
    "            text = text,\n",
    "            style='Sad',\n",
    "            style_weight=1,\n",
    "            split_interval = 0.3,\n",
    "            use_assist_text = True if assist_text is not None else None,\n",
    "            assist_text = assist_text\n",
    "        )\n",
    "    \n",
    "    return sr, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77c49065-436f-4aff-8207-b1756737c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def gen_audio_dialogue(text_dialogue_list, prompt):\n",
    "    # 音声ファイルを順番に生成（ファイルは不要なのでwave配列で持つ）\n",
    "    wav_data = []\n",
    "    for dial in text_dialogue_list:\n",
    "        speaker = dial.speaker\n",
    "        sr, wav = sbv_tts(dial.text, speaker, prompt)\n",
    "\n",
    "        # サンプリングレートを変換\n",
    "        if sr != setting_sr:\n",
    "            # 16ビット整数のデータを、-1.0から1.0の範囲に収まる浮動小数点数に正規化\n",
    "            wav = wav.astype(np.float32) / 32768.0\n",
    "            wav = librosa.resample(wav, orig_sr=sr, target_sr=setting_sr)\n",
    "\n",
    "        # 0.3秒間の無音時間を追加\n",
    "        duration_sec = 0.3\n",
    "        num_silent_samples = int(setting_sr*duration_sec)\n",
    "        silence = np.zeros(num_silent_samples, dtype=wav.dtype)\n",
    "        wav_with_silence = np.concatenate((wav, silence))\n",
    "        wav_data.append(wav_with_silence)\n",
    "    \n",
    "    # 最終的な音声長を決定\n",
    "    max_len = sum([len(w) for w in wav_data])\n",
    "    \n",
    "    # ステレオ音声用（2チャンネル×最大長）の空配列をゼロ初期化で作成\n",
    "    stereo = np.zeros((2, max_len), dtype=np.float32)\n",
    "    \n",
    "    pos = 0\n",
    "    for i, wav in enumerate(wav_data):\n",
    "        ch = i%2  # 0:左(A), 1:右(B)\n",
    "        stereo[ch, pos:pos+len(wav)] += wav\n",
    "        pos += len(wav)\n",
    "    \n",
    "    # 転置(-1,2)する\n",
    "    stereo = stereo.T\n",
    "    return stereo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea61cce2-6688-4c34-a8cd-c7465a350ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text(file_name, text_dialogue_list):\n",
    "    result = \"\"\n",
    "    for dial in text_dialogue_list:\n",
    "        result += dial.text + \"\\n\"\n",
    "    with open(file_name, \"w\") as f:\n",
    "        f.write(result)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ca58ce7-d40d-4a18-98b5-bd6576f7bc06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m11-05 23:34:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "こんにちは、Bさん。今日はどんな様子でしたか？何か気になっていることはありますか？\n",
      "\u001b[32m11-05 23:34:28\u001b[0m |\u001b[1m  INFO  \u001b[0m| infer.py:24 | Using JP-Extra model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs1/s1f102201582/anaconda3/envs/mfa_py311/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m11-05 23:34:36\u001b[0m |\u001b[1m  INFO  \u001b[0m| safetensors.py:50 | Loaded 'model_assets/jvnv-F2-jp/jvnv-F2_e166_s20000.safetensors' (iteration 166)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs1/s1f102201582/anaconda3/envs/mfa_py311/lib/python3.11/site-packages/pyopenjtalk/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m audio_synth_prompt = build_audio_synth_prompt(text_dialogue_list)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 対話テキストを音声合成\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m stereo = \u001b[43mgen_audio_dialogue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_dialogue_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_synth_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m wav_name = \u001b[33m\"\u001b[39m\u001b[33m./mfa_test_in/test.wav\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# wavファイル出力\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mgen_audio_dialogue\u001b[39m\u001b[34m(text_dialogue_list, prompt)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dial \u001b[38;5;129;01min\u001b[39;00m text_dialogue_list:\n\u001b[32m      8\u001b[39m     speaker = dial.speaker\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     sr, wav = \u001b[43msbv_tts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdial\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# サンプリングレートを変換\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sr != setting_sr:\n\u001b[32m     13\u001b[39m         \u001b[38;5;66;03m# 16ビット整数のデータを、-1.0から1.0の範囲に収まる浮動小数点数に正規化\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36msbv_tts\u001b[39m\u001b[34m(text, speaker, assist_text)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msbv_tts\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, speaker: Literal[\u001b[33m\"\u001b[39m\u001b[33mA\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m], assist_text=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m speaker == \u001b[33m\"\u001b[39m\u001b[33mA\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         sr, audio = \u001b[43mA_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHappy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstyle_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[43msplit_interval\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_assist_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43massist_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[43massist_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43massist_text\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     14\u001b[39m         sr, audio = B_model.infer(\n\u001b[32m     15\u001b[39m             text = text,\n\u001b[32m     16\u001b[39m             style=\u001b[33m'\u001b[39m\u001b[33mSad\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m             assist_text = assist_text\n\u001b[32m     21\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/mfa_py311/lib/python3.11/site-packages/style_bert_vits2/tts_model.py:305\u001b[39m, in \u001b[36mTTSModel.infer\u001b[39m\u001b[34m(self, text, language, speaker_id, reference_audio_path, sdp_ratio, noise, noise_w, length, line_split, split_interval, assist_text, assist_text_weight, use_assist_text, style, style_weight, given_phone, given_tone, pitch_scale, intonation_scale)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(texts):\n\u001b[32m    304\u001b[39m         audios.append(\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m             \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m                \u001b[49m\u001b[43msdp_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43msdp_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m                \u001b[49m\u001b[43mnoise_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m                \u001b[49m\u001b[43mnoise_scale_w\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnoise_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m                \u001b[49m\u001b[43mlength_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m                \u001b[49m\u001b[43msid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m                \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m                \u001b[49m\u001b[43mhps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m                \u001b[49m\u001b[43mnet_g\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__net_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m                \u001b[49m\u001b[43massist_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43massist_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m                \u001b[49m\u001b[43massist_text_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43massist_text_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstyle_vec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstyle_vector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m         )\n\u001b[32m    321\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[32m1\u001b[39m:\n\u001b[32m    322\u001b[39m             audios.append(np.zeros(\u001b[38;5;28mint\u001b[39m(\u001b[32m44100\u001b[39m * split_interval)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/mfa_py311/lib/python3.11/site-packages/style_bert_vits2/models/infer.py:221\u001b[39m, in \u001b[36minfer\u001b[39m\u001b[34m(text, style_vec, sdp_ratio, noise_scale, noise_scale_w, length_scale, sid, language, hps, net_g, device, skip_start, skip_end, assist_text, assist_text_weight, given_phone, given_tone)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minfer\u001b[39m(\n\u001b[32m    202\u001b[39m     text: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    203\u001b[39m     style_vec: NDArray[Any],\n\u001b[32m   (...)\u001b[39m\u001b[32m    218\u001b[39m     given_tone: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    219\u001b[39m ):\n\u001b[32m    220\u001b[39m     is_jp_extra = hps.version.endswith(\u001b[33m\"\u001b[39m\u001b[33mJP-Extra\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     bert, ja_bert, en_bert, phones, tones, lang_ids = \u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43massist_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43massist_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43massist_text_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43massist_text_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgiven_phone\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgiven_phone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgiven_tone\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgiven_tone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m skip_start:\n\u001b[32m    232\u001b[39m         phones = phones[\u001b[32m3\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/mfa_py311/lib/python3.11/site-packages/style_bert_vits2/models/infer.py:110\u001b[39m, in \u001b[36mget_text\u001b[39m\u001b[34m(text, language_str, hps, device, assist_text, assist_text_weight, given_phone, given_tone)\u001b[39m\n\u001b[32m    108\u001b[39m use_jp_extra = hps.version.endswith(\u001b[33m\"\u001b[39m\u001b[33mJP-Extra\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# 推論時のみ呼び出されるので、raise_yomi_error は False に設定\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m norm_text, phone, tone, word2ph = \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_jp_extra\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_jp_extra\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraise_yomi_error\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# phone と tone の両方が与えられた場合はそれを使う\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m given_phone \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m given_tone \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# 指定された phone と指定された tone 両方の長さが一致していなければならない\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/mfa_py311/lib/python3.11/site-packages/style_bert_vits2/nlp/__init__.py:80\u001b[39m, in \u001b[36mclean_text\u001b[39m\u001b[34m(text, language, use_jp_extra, raise_yomi_error)\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstyle_bert_vits2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnlp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjapanese\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnormalizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_text\n\u001b[32m     79\u001b[39m     norm_text = normalize_text(text)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     phones, tones, word2ph = \u001b[43mg2p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_jp_extra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_yomi_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m language == Languages.EN:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstyle_bert_vits2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnlp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menglish\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mg2p\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m g2p\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/mfa_py311/lib/python3.11/site-packages/style_bert_vits2/nlp/japanese/g2p.py:40\u001b[39m, in \u001b[36mg2p\u001b[39m\u001b[34m(norm_text, use_jp_extra, raise_yomi_error)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m他で使われるメインの関数。`normalize_text()` で正規化された `norm_text` を受け取り、\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m- phones: 音素のリスト（ただし `!` や `,` や `.` など punctuation が含まれうる）\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m \u001b[33;03m    tuple[list[str], list[int], list[int]]: 音素のリスト、アクセントのリスト、word2ph のリスト\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# pyopenjtalk のフルコンテキストラベルを使ってアクセントを取り出すと、punctuation の位置が消えてしまい情報が失われてしまう：\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# 「こんにちは、世界。」と「こんにちは！世界。」と「こんにちは！！！？？？世界……。」は全て同じになる。\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# よって、まず punctuation 無しの音素とアクセントのリストを作り、\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# punctuation がすべて消えた、音素とアクセントのタプルのリスト（「ん」は「N」）\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m phone_tone_list_wo_punct = \u001b[43m__g2phone_tone_wo_punct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# sep_text: 単語単位の単語のリスト\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# sep_kata: 単語単位の単語のカタカナ読みのリスト、読めない文字は raise_yomi_error=True なら例外、False なら読めない文字を「'」として返ってくる\u001b[39;00m\n\u001b[32m     44\u001b[39m sep_text, sep_kata = text_to_sep_kata(norm_text, raise_yomi_error=raise_yomi_error)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/mfa_py311/lib/python3.11/site-packages/style_bert_vits2/nlp/japanese/g2p.py:390\u001b[39m, in \u001b[36m__g2phone_tone_wo_punct\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__g2phone_tone_wo_punct\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m    375\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[33;03m    テキストに対して、音素とアクセント（0か1）のペアのリストを返す。\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[33;03m    ただし「!」「.」「?」等の非音素記号 (punctuation) は全て消える（ポーズ記号も残さない）。\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    387\u001b[39m \u001b[33;03m        list[tuple[str, int]]: 音素とアクセントのペアのリスト\u001b[39;00m\n\u001b[32m    388\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     prosodies = \u001b[43m__pyopenjtalk_g2p_prosody\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_unvoiced_vowels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m     \u001b[38;5;66;03m# logger.debug(f\"prosodies: {prosodies}\")\u001b[39;00m\n\u001b[32m    392\u001b[39m     result: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/mfa_py311/lib/python3.11/site-packages/style_bert_vits2/nlp/japanese/g2p.py:474\u001b[39m, in \u001b[36m__pyopenjtalk_g2p_prosody\u001b[39m\u001b[34m(text, drop_unvoiced_vowels)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[32m50\u001b[39m\n\u001b[32m    472\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(match.group(\u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m labels = pyopenjtalk.make_label(\u001b[43mpyopenjtalk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_frontend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    475\u001b[39m N = \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[32m    477\u001b[39m phones = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/mfa_py311/lib/python3.11/site-packages/style_bert_vits2/nlp/japanese/pyopenjtalk_worker/__init__.py:27\u001b[39m, in \u001b[36mrun_frontend\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# without worker\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyopenjtalk\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pyopenjtalk.run_frontend(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/mfa_py311/lib/python3.11/site-packages/pyopenjtalk/__init__.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mBUG: version.py doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist. Please file a bug report.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhtsengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTSEngine\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopenjtalk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenJTalk\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopenjtalk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mecab_dict_index \u001b[38;5;28;01mas\u001b[39;00m _mecab_dict_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyopenjtalk/htsengine.pyx:1\u001b[39m, in \u001b[36minit pyopenjtalk.htsengine\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "text_dialogue_list = gen_txt_dialogue()\n",
    "audio_synth_prompt = build_audio_synth_prompt(text_dialogue_list)\n",
    "# 対話テキストを音声合成\n",
    "stereo = gen_audio_dialogue(text_dialogue_list, audio_synth_prompt)\n",
    "\n",
    "wav_name = \"./mfa_test_in/test.wav\"\n",
    "# wavファイル出力\n",
    "sf.write(wav_name, stereo, setting_sr)\n",
    "\n",
    "text_file_name = \"./mfa_test_in/test.txt\"\n",
    "write_text(text_file_name, text_dialogue_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d07ef028-537f-4fe0-9ec7-3317e6c53b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to mfa_test_out\u001b[33m...\u001b[0m                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to mfa_test_out!                         \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m163.017\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 230 ms, sys: 106 ms, total: 337 ms\n",
      "Wall time: 2min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mfa', 'align', '--quiet', '--fine_tune', '--overwrite', '--clean', '--final_clean', '--output_format', 'json', './mfa_test_in/', 'japanese_mfa', '/users/s1f102201582/Documents/MFA/pretrained_models/acoustic/japanese_mfa.zip', './mfa_test_out/', '--beam', '1000', '--retry_beam', '4000', '--punctuation', '\"…\"'], returncode=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import subprocess\n",
    "from os.path import expanduser, join\n",
    "\n",
    "home_dir = expanduser(\"~\")\n",
    "model_dir = join(home_dir, \"Documents/MFA/pretrained_models/acoustic/japanese_mfa.zip\")\n",
    "\n",
    "subprocess.run([\n",
    "    \"mfa\",\n",
    "    \"align\",\n",
    "    \"--quiet\",\n",
    "    \"--fine_tune\", \n",
    "    \"--overwrite\",\n",
    "    \"--clean\",\n",
    "    \"--final_clean\",\n",
    "    \"--output_format\", \"json\",\n",
    "    \"./mfa_test_in/\",\n",
    "    \"japanese_mfa\",\n",
    "    model_dir,\n",
    "    \"./mfa_test_out/\",\n",
    "    \"--beam\", \"1000\",\n",
    "    \"--retry_beam\", \"4000\",\n",
    "    \"--punctuation\", '\"…\"',\n",
    "])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5f1a2-6eee-40e1-b07a-46ba30ce6a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !mfa validate --ignore_acoustics ./test/ japanese_mfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ac824-2294-421c-955f-bddf95caf7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
