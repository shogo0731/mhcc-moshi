{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ams-19wF8zgY"
   },
   "source": [
    "## Prepare dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5085 files: 100%|██████████████████████████████████████████████████████████| 5085/5085 [01:11<00:00, 71.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "dataset_path = \"/users/s1f102201582/projects/mhcc-moshi/moshi/data/daily-talk-contiguous\"\n",
    "\n",
    "Path(dataset_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the dataset\n",
    "local_dir = snapshot_download(\n",
    "    \"kyutai/DailyTalkContiguous\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=dataset_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hia7n0T1_mHZ"
   },
   "source": [
    "## Start training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZtcLerooWFeB"
   },
   "outputs": [],
   "source": [
    "# these info is needed for training\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ErD1ktQUMyPZ"
   },
   "outputs": [],
   "source": [
    "# make sure the run_dir has not been created before\n",
    "# only run this when you ran torchrun previously and created the /content/test_ultra file\n",
    "! rm -r /users/s1f102201582/projects/mhcc-moshi/moshi/output/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4wFgmwIUTtg",
    "outputId": "8fe22185-6e12-4987-c4f6-3768952cec7c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1115 23:01:21.043000 1105485 site-packages/torch/distributed/run.py:792] \n",
      "W1115 23:01:21.043000 1105485 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W1115 23:01:21.043000 1105485 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1115 23:01:21.043000 1105485 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "Warning: `hf_repo_id` is set but `config_path` is None. This will load default models.\n",
      "Warning: `hf_repo_id` is set but `config_path` is None. This will load default models.\n",
      "Warning: `hf_repo_id` is set but `config_path` is None. This will load default models.\n",
      "Warning: `hf_repo_id` is set but `config_path` is None. This will load default models.\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - torch.cuda.device_count: 4\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - CUDA_VISIBLE_DEVICES: 0,1,2,3\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - local rank: 0\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - torch.cuda.device_count: 4\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - CUDA_VISIBLE_DEVICES: 0,1,2,3\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - local rank: 3\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - torch.cuda.device_count: 4\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - CUDA_VISIBLE_DEVICES: 0,1,2,3\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - local rank: 2\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - torch.cuda.device_count: 4\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - CUDA_VISIBLE_DEVICES: 0,1,2,3\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - local rank: 1\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - Set cuda device to 0\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - train - INFO - Going to init comms...\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - Set cuda device to 3\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - train - INFO - Run dir: /users/s1f102201582/projects/mhcc-moshi/moshi/output/test\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - Set cuda device to 2\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - distributed - INFO - Set cuda device to 1\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - train - INFO - Going to init comms...\n",
      "[rank2]:[W1115 23:01:25.536649592 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank0]:[W1115 23:01:25.555368778 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - train - INFO - Going to init comms...\n",
      "[rank3]:[W1115 23:01:25.587453718 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "2025-11-15 23:01:25 (JST) - 0:00:04 - train - INFO - Going to init comms...\n",
      "[rank1]:[W1115 23:01:25.601696861 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "2025-11-15 23:01:26 (JST) - 0:00:04 - train - INFO - TrainArgs: {'batch_size': 4,\n",
      " 'ckpt_freq': 10,\n",
      " 'data': {'eval_data': '',\n",
      "          'shuffle': True,\n",
      "          'train_data': '/users/s1f102201582/projects/mhcc-moshi/moshi/data/daily-talk-contiguous/dailytalk.jsonl'},\n",
      " 'do_ckpt': True,\n",
      " 'do_eval': False,\n",
      " 'duration_sec': 300.0,\n",
      " 'eval_freq': 1,\n",
      " 'first_codebook_weight_multiplier': 100.0,\n",
      " 'full_finetuning': False,\n",
      " 'gradient_checkpointing': True,\n",
      " 'log_freq': 10,\n",
      " 'lora': {'enable': True, 'ft_embed': False, 'rank': 128, 'scaling': 2.0},\n",
      " 'max_norm': 1.0,\n",
      " 'max_steps': 300,\n",
      " 'moshi_paths': {'config_path': None,\n",
      "                 'hf_repo_id': 'kyutai/moshiko-pytorch-bf16',\n",
      "                 'mimi_path': None,\n",
      "                 'moshi_path': None,\n",
      "                 'tokenizer_path': None},\n",
      " 'num_ckpt_keep': 3,\n",
      " 'num_microbatches': 1,\n",
      " 'optim': {'lr': 2e-06, 'pct_start': 0.05, 'weight_decay': 0.1},\n",
      " 'overwrite_run_dir': False,\n",
      " 'param_dtype': 'bfloat16',\n",
      " 'run_dir': '/users/s1f102201582/projects/mhcc-moshi/moshi/output/test',\n",
      " 'save_adapters': True,\n",
      " 'seed': 0,\n",
      " 'text_padding_weight': 0.5,\n",
      " 'wandb': {'key': None, 'offline': False, 'project': None, 'run_name': None},\n",
      " 'world_size': 4}\n",
      "2025-11-15 23:01:26 (JST) - 0:00:04 - train - INFO - Loading Mimi and Moshi...\n",
      "/nfs1/s1f102201582/anaconda3/envs/moshi/lib/python3.12/site-packages/moshi/models/loaders.py:202: UserWarning: Repository kyutai/moshiko-pytorch-bf16 contains no config.json. Assuming this is a Moshi 7B. Support for such repository might be removed in the future.\n",
      "  warnings.warn(\n",
      "/nfs1/s1f102201582/anaconda3/envs/moshi/lib/python3.12/site-packages/moshi/models/loaders.py:202: UserWarning: Repository kyutai/moshiko-pytorch-bf16 contains no config.json. Assuming this is a Moshi 7B. Support for such repository might be removed in the future.\n",
      "  warnings.warn(\n",
      "/nfs1/s1f102201582/anaconda3/envs/moshi/lib/python3.12/site-packages/moshi/models/loaders.py:202: UserWarning: Repository kyutai/moshiko-pytorch-bf16 contains no config.json. Assuming this is a Moshi 7B. Support for such repository might be removed in the future.\n",
      "  warnings.warn(\n",
      "/nfs1/s1f102201582/anaconda3/envs/moshi/lib/python3.12/site-packages/moshi/models/loaders.py:202: UserWarning: Repository kyutai/moshiko-pytorch-bf16 contains no config.json. Assuming this is a Moshi 7B. Support for such repository might be removed in the future.\n",
      "  warnings.warn(\n",
      "2025-11-15 23:01:27 (JST) - 0:00:06 - finetune.wrapped_model - INFO - Converting model to dtype torch.bfloat16 ...\n",
      "2025-11-15 23:01:27 (JST) - 0:00:06 - finetune.wrapped_model - INFO - Initializing lora layers ...\n",
      "2025-11-15 23:01:29 (JST) - 0:00:08 - finetune.wrapped_model - INFO - Finished initialization!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/moshi/lib/python3.12/site-packages/IPython/utils/_process_posix.py:130\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     res_idx = \u001b[43mchild\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28mprint\u001b[39m(child.before[out_size:].decode(enc, \u001b[33m'\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m'\u001b[39m), end=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/moshi/lib/python3.12/site-packages/pexpect/spawnbase.py:383\u001b[39m, in \u001b[36mSpawnBase.expect_list\u001b[39m\u001b[34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/moshi/lib/python3.12/site-packages/pexpect/expect.py:169\u001b[39m, in \u001b[36mExpecter.expect_loop\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m incoming = \u001b[43mspawn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_nonblocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaxread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.spawn.delayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/moshi/lib/python3.12/site-packages/pexpect/pty_spawn.py:500\u001b[39m, in \u001b[36mspawn.read_nonblocking\u001b[39m\u001b[34m(self, size, timeout)\u001b[39m\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (timeout != \u001b[32m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m).read_nonblocking(size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/moshi/lib/python3.12/site-packages/pexpect/pty_spawn.py:450\u001b[39m, in \u001b[36mspawn.read_nonblocking.<locals>.select\u001b[39m\u001b[34m(timeout)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_ignore_interrupts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchild_fd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/moshi/lib/python3.12/site-packages/pexpect/utils.py:143\u001b[39m, in \u001b[36mselect_ignore_interrupts\u001b[39m\u001b[34m(iwtd, owtd, ewtd, timeout)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43miwtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# start training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcd /users/s1f102201582/projects/moshi-finetune && torchrun --nproc-per-node 4 -m train /users/s1f102201582/projects/mhcc-moshi/moshi/generate_data/sandbox/example.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/moshi/lib/python3.12/site-packages/ipykernel/zmqshell.py:788\u001b[39m, in \u001b[36mZMQInteractiveShell.system_piped\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    786\u001b[39m         \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = system(cmd)\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/moshi/lib/python3.12/site-packages/IPython/utils/_process_posix.py:154\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    151\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    153\u001b[39m         \u001b[38;5;66;03m# Ensure the subprocess really is terminated\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m         \u001b[43mchild\u001b[49m\u001b[43m.\u001b[49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# add isalive check, to ensure exitstatus is set:\u001b[39;00m\n\u001b[32m    156\u001b[39m child.isalive()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs1/s1f102201582/anaconda3/envs/moshi/lib/python3.12/site-packages/pexpect/pty_spawn.py:650\u001b[39m, in \u001b[36mspawn.terminate\u001b[39m\u001b[34m(self, force)\u001b[39m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    649\u001b[39m \u001b[38;5;28mself\u001b[39m.kill(signal.SIGINT)\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdelayafterterminate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.isalive():\n\u001b[32m    652\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "\n",
    "!cd /users/s1f102201582/projects/moshi-finetune && torchrun --nproc-per-node 4 -m train /users/s1f102201582/projects/mhcc-moshi/moshi/generate_data/sandbox/example.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
