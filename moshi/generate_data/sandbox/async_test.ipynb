{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "288efb08-5a11-449e-addd-d9ae5d024f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"これはサンプルテキストです\"\n",
    "prompt_text = \"元気に\"\n",
    "prompt_tokens_path = \"/nfs1/s1f102201582/projects/mhcc-moshi/moshi/fake.npy\"\n",
    "semantic_path = \"./temp/codes_0.npy\"\n",
    "out_path = \"./temp/fake.wav\"\n",
    "fs_tmp_path = \"./temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e87cf6b-5617-488c-be2f-bbc85f4ba055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import functools\n",
    "\n",
    "def task_B(task: asyncio.Task):\n",
    "    \"\"\"動的に追加されるタスク\"\"\"\n",
    "    print(task.result())\n",
    "\n",
    "async def task_A(semaphore, i):\n",
    "    async with semaphore:\n",
    "        print(f\"start process: {i}\")\n",
    "        cmd = f\"\"\"python /nfs1/s1f102201582/projects/fish-speech/fish_speech/models/text2semantic/inference.py --text {text} --prompt-text {prompt_text} --prompt-tokens {prompt_tokens_path} --output-dir {fs_tmp_path} --checkpoint-path /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/\"\"\"\n",
    "        print(cmd)\n",
    "        process = await asyncio.create_subprocess_shell(\n",
    "            cmd,\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE\n",
    "        )\n",
    "        stdout, stderr = await process.communicate()\n",
    "    return stdout, stderr\n",
    "\n",
    "async def main():\n",
    "    semaphore = asyncio.Semaphore(5)\n",
    "    all_tasks = []\n",
    "    for i in range(10):\n",
    "        cor = task_A(semaphore, i)\n",
    "        task = asyncio.create_task(cor)\n",
    "        task.add_done_callback(functools.partial(task_B))\n",
    "        all_tasks.append(task)\n",
    "    await asyncio.gather(*all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "708ffba1-aaef-403f-8b64-54a8182f6608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:12:56] asyncio.run(main()) を呼び出します\n",
      "start process: 0\n",
      "python /nfs1/s1f102201582/projects/fish-speech/fish_speech/models/text2semantic/inference.py --text これはサンプルテキストです --prompt-text 元気に --prompt-tokens /nfs1/s1f102201582/projects/mhcc-moshi/moshi/fake.npy --output-dir ./temp --checkpoint-path /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/\n",
      "start process: 1\n",
      "python /nfs1/s1f102201582/projects/fish-speech/fish_speech/models/text2semantic/inference.py --text これはサンプルテキストです --prompt-text 元気に --prompt-tokens /nfs1/s1f102201582/projects/mhcc-moshi/moshi/fake.npy --output-dir ./temp --checkpoint-path /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/\n",
      "start process: 2\n",
      "python /nfs1/s1f102201582/projects/fish-speech/fish_speech/models/text2semantic/inference.py --text これはサンプルテキストです --prompt-text 元気に --prompt-tokens /nfs1/s1f102201582/projects/mhcc-moshi/moshi/fake.npy --output-dir ./temp --checkpoint-path /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/\n",
      "start process: 3\n",
      "python /nfs1/s1f102201582/projects/fish-speech/fish_speech/models/text2semantic/inference.py --text これはサンプルテキストです --prompt-text 元気に --prompt-tokens /nfs1/s1f102201582/projects/mhcc-moshi/moshi/fake.npy --output-dir ./temp --checkpoint-path /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/\n",
      "start process: 4\n",
      "python /nfs1/s1f102201582/projects/fish-speech/fish_speech/models/text2semantic/inference.py --text これはサンプルテキストです --prompt-text 元気に --prompt-tokens /nfs1/s1f102201582/projects/mhcc-moshi/moshi/fake.npy --output-dir ./temp --checkpoint-path /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/\n",
      "start process: 5\n",
      "python /nfs1/s1f102201582/projects/fish-speech/fish_speech/models/text2semantic/inference.py --text これはサンプルテキストです --prompt-text 元気に --prompt-tokens /nfs1/s1f102201582/projects/mhcc-moshi/moshi/fake.npy --output-dir ./temp --checkpoint-path /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/\n",
      "(b'', b\"2025-11-18 22:13:02.059 | INFO     | __main__:main:644 - Loading model ...\\n2025-11-18 22:13:02.288 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:432 - Loading model from /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini, config: DualARModelArgs(model_type='dual_ar', vocab_size=155776, n_layer=28, n_head=16, dim=1024, intermediate_size=3072, n_local_heads=8, head_dim=128, rope_base=1000000, norm_eps=1e-06, max_seq_len=8192, dropout=0.0, tie_word_embeddings=False, attention_qkv_bias=False, attention_o_bias=False, attention_qk_norm=True, codebook_size=4096, num_codebooks=10, use_gradient_checkpointing=True, initializer_range=0.03125, is_reward_model=False, scale_codebook_embeddings=True, n_fast_layer=4, fast_dim=1024, fast_n_head=16, fast_n_local_heads=8, fast_head_dim=64, fast_intermediate_size=3072, fast_attention_qkv_bias=False, fast_attention_qk_norm=False, fast_attention_o_bias=False)\\n2025-11-18 22:13:10.631 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:494 - Model weights loaded - Status: <All keys matched successfully>\\n2025-11-18 22:13:13.167 | INFO     | __main__:init_model:357 - Restored model from checkpoint\\n2025-11-18 22:13:13.167 | INFO     | __main__:init_model:362 - Using DualARTransformer\\n2025-11-18 22:13:13.185 | INFO     | __main__:main:658 - Time to load model: 11.13 seconds\\n2025-11-18 22:13:13.210 | INFO     | __main__:generate_long:457 - Encoded text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n\\r  0%|          | 0/4289 [00:00<?, ?it/s]\\r  0%|          | 1/4289 [00:00<11:52,  6.02it/s]\\r  0%|          | 2/4289 [00:00<11:36,  6.16it/s]\\r  0%|          | 3/4289 [00:00<12:07,  5.89it/s]\\r  0%|          | 4/4289 [00:00<12:45,  5.60it/s]\\r  0%|          | 5/4289 [00:00<13:11,  5.41it/s]\\r  0%|          | 6/4289 [00:01<13:28,  5.29it/s]\\r  0%|          | 7/4289 [00:01<13:39,  5.22it/s]\\r  0%|          | 8/4289 [00:01<13:46,  5.18it/s]\\r  0%|          | 9/4289 [00:01<13:51,  5.15it/s]\\r  0%|          | 10/4289 [00:01<13:53,  5.13it/s]\\r  0%|          | 11/4289 [00:02<13:55,  5.12it/s]\\r  0%|          | 12/4289 [00:02<13:56,  5.11it/s]\\r  0%|          | 13/4289 [00:02<13:57,  5.10it/s]\\r  0%|          | 14/4289 [00:02<13:58,  5.10it/s]\\r  0%|          | 15/4289 [00:02<13:59,  5.09it/s]\\r  0%|          | 16/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 17/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 18/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 19/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 20/4289 [00:03<13:58,  5.09it/s]\\r  0%|          | 21/4289 [00:04<13:58,  5.09it/s]\\r  1%|          | 22/4289 [00:04<13:59,  5.09it/s]\\r  1%|          | 23/4289 [00:04<13:58,  5.09it/s]\\r  1%|          | 24/4289 [00:04<13:58,  5.09it/s]\\r  1%|          | 25/4289 [00:04<13:58,  5.08it/s]\\r  1%|          | 26/4289 [00:05<13:59,  5.08it/s]\\r  1%|          | 27/4289 [00:05<13:58,  5.08it/s]\\r  1%|          | 28/4289 [00:05<13:58,  5.08it/s]\\r  1%|          | 29/4289 [00:05<13:58,  5.08it/s]\\r  1%|          | 30/4289 [00:05<13:58,  5.08it/s]\\r  1%|          | 31/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 32/4289 [00:06<13:57,  5.09it/s]\\r  1%|          | 33/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 34/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 35/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 35/4289 [00:06<14:09,  5.01it/s]\\n2025-11-18 22:13:21.427 | INFO     | __main__:generate_long:491 - Generated 37 tokens in 8.22 seconds, 4.50 tokens/sec\\n2025-11-18 22:13:21.427 | INFO     | __main__:generate_long:494 - Bandwidth achieved: 3.87 GB/s\\n2025-11-18 22:13:21.427 | INFO     | __main__:generate_long:497 - GPU Memory used: 3.32 GB\\n2025-11-18 22:13:21.444 | INFO     | __main__:main:692 - Sampled text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n2025-11-18 22:13:21.476 | INFO     | __main__:main:697 - Saved codes to temp/codes_0.npy\\n2025-11-18 22:13:21.476 | INFO     | __main__:main:698 - Next sample\\n\")\n",
      "start process: 6\n",
      "python /nfs1/s1f102201582/projects/fish-speech/fish_speech/models/text2semantic/inference.py --text これはサンプルテキストです --prompt-text 元気に --prompt-tokens /nfs1/s1f102201582/projects/mhcc-moshi/moshi/fake.npy --output-dir ./temp --checkpoint-path /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/\n",
      "(b'', b\"2025-11-18 22:13:02.059 | INFO     | __main__:main:644 - Loading model ...\\n2025-11-18 22:13:02.289 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:432 - Loading model from /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini, config: DualARModelArgs(model_type='dual_ar', vocab_size=155776, n_layer=28, n_head=16, dim=1024, intermediate_size=3072, n_local_heads=8, head_dim=128, rope_base=1000000, norm_eps=1e-06, max_seq_len=8192, dropout=0.0, tie_word_embeddings=False, attention_qkv_bias=False, attention_o_bias=False, attention_qk_norm=True, codebook_size=4096, num_codebooks=10, use_gradient_checkpointing=True, initializer_range=0.03125, is_reward_model=False, scale_codebook_embeddings=True, n_fast_layer=4, fast_dim=1024, fast_n_head=16, fast_n_local_heads=8, fast_head_dim=64, fast_intermediate_size=3072, fast_attention_qkv_bias=False, fast_attention_qk_norm=False, fast_attention_o_bias=False)\\n2025-11-18 22:13:10.610 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:494 - Model weights loaded - Status: <All keys matched successfully>\\n2025-11-18 22:13:13.236 | INFO     | __main__:init_model:357 - Restored model from checkpoint\\n2025-11-18 22:13:13.236 | INFO     | __main__:init_model:362 - Using DualARTransformer\\n2025-11-18 22:13:13.254 | INFO     | __main__:main:658 - Time to load model: 11.19 seconds\\n2025-11-18 22:13:13.280 | INFO     | __main__:generate_long:457 - Encoded text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n\\r  0%|          | 0/4289 [00:00<?, ?it/s]\\r  0%|          | 1/4289 [00:00<18:12,  3.93it/s]\\r  0%|          | 2/4289 [00:00<15:31,  4.60it/s]\\r  0%|          | 3/4289 [00:00<14:50,  4.81it/s]\\r  0%|          | 4/4289 [00:00<14:31,  4.92it/s]\\r  0%|          | 5/4289 [00:01<14:20,  4.98it/s]\\r  0%|          | 6/4289 [00:01<14:14,  5.01it/s]\\r  0%|          | 7/4289 [00:01<14:09,  5.04it/s]\\r  0%|          | 8/4289 [00:01<14:07,  5.05it/s]\\r  0%|          | 9/4289 [00:01<14:04,  5.07it/s]\\r  0%|          | 10/4289 [00:02<14:03,  5.07it/s]\\r  0%|          | 11/4289 [00:02<14:02,  5.08it/s]\\r  0%|          | 12/4289 [00:02<14:01,  5.08it/s]\\r  0%|          | 13/4289 [00:02<14:02,  5.08it/s]\\r  0%|          | 14/4289 [00:02<14:01,  5.08it/s]\\r  0%|          | 15/4289 [00:03<14:00,  5.08it/s]\\r  0%|          | 16/4289 [00:03<14:00,  5.09it/s]\\r  0%|          | 17/4289 [00:03<14:00,  5.08it/s]\\r  0%|          | 18/4289 [00:03<14:00,  5.08it/s]\\r  0%|          | 19/4289 [00:03<13:58,  5.09it/s]\\r  0%|          | 20/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 21/4289 [00:04<13:59,  5.09it/s]\\r  1%|          | 22/4289 [00:04<13:59,  5.08it/s]\\r  1%|          | 23/4289 [00:04<13:58,  5.09it/s]\\r  1%|          | 24/4289 [00:04<13:59,  5.08it/s]\\r  1%|          | 25/4289 [00:04<13:58,  5.08it/s]\\r  1%|          | 26/4289 [00:05<13:58,  5.09it/s]\\r  1%|          | 27/4289 [00:05<13:58,  5.08it/s]\\r  1%|          | 28/4289 [00:05<13:58,  5.08it/s]\\r  1%|          | 29/4289 [00:05<13:57,  5.09it/s]\\r  1%|          | 30/4289 [00:05<13:57,  5.09it/s]\\r  1%|          | 31/4289 [00:06<13:57,  5.09it/s]\\r  1%|          | 32/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 33/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 34/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 35/4289 [00:06<13:25,  5.28it/s]\\r  1%|          | 35/4289 [00:07<14:18,  4.95it/s]\\n2025-11-18 22:13:21.639 | INFO     | __main__:generate_long:491 - Generated 37 tokens in 8.36 seconds, 4.43 tokens/sec\\n2025-11-18 22:13:21.640 | INFO     | __main__:generate_long:494 - Bandwidth achieved: 3.81 GB/s\\n2025-11-18 22:13:21.640 | INFO     | __main__:generate_long:497 - GPU Memory used: 3.32 GB\\n2025-11-18 22:13:21.644 | INFO     | __main__:main:692 - Sampled text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n2025-11-18 22:13:21.650 | INFO     | __main__:main:697 - Saved codes to temp/codes_0.npy\\n2025-11-18 22:13:21.650 | INFO     | __main__:main:698 - Next sample\\n\")\n",
      "start process: 7\n",
      "python /nfs1/s1f102201582/projects/fish-speech/fish_speech/models/text2semantic/inference.py --text これはサンプルテキストです --prompt-text 元気に --prompt-tokens /nfs1/s1f102201582/projects/mhcc-moshi/moshi/fake.npy --output-dir ./temp --checkpoint-path /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/\n",
      "(b'', b\"2025-11-18 22:13:02.059 | INFO     | __main__:main:644 - Loading model ...\\n2025-11-18 22:13:02.289 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:432 - Loading model from /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini, config: DualARModelArgs(model_type='dual_ar', vocab_size=155776, n_layer=28, n_head=16, dim=1024, intermediate_size=3072, n_local_heads=8, head_dim=128, rope_base=1000000, norm_eps=1e-06, max_seq_len=8192, dropout=0.0, tie_word_embeddings=False, attention_qkv_bias=False, attention_o_bias=False, attention_qk_norm=True, codebook_size=4096, num_codebooks=10, use_gradient_checkpointing=True, initializer_range=0.03125, is_reward_model=False, scale_codebook_embeddings=True, n_fast_layer=4, fast_dim=1024, fast_n_head=16, fast_n_local_heads=8, fast_head_dim=64, fast_intermediate_size=3072, fast_attention_qkv_bias=False, fast_attention_qk_norm=False, fast_attention_o_bias=False)\\n2025-11-18 22:13:10.571 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:494 - Model weights loaded - Status: <All keys matched successfully>\\n2025-11-18 22:13:13.302 | INFO     | __main__:init_model:357 - Restored model from checkpoint\\n2025-11-18 22:13:13.302 | INFO     | __main__:init_model:362 - Using DualARTransformer\\n2025-11-18 22:13:13.321 | INFO     | __main__:main:658 - Time to load model: 11.26 seconds\\n2025-11-18 22:13:13.344 | INFO     | __main__:generate_long:457 - Encoded text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n\\r  0%|          | 0/4289 [00:00<?, ?it/s]\\r  0%|          | 1/4289 [00:00<18:37,  3.84it/s]\\r  0%|          | 2/4289 [00:00<16:08,  4.43it/s]\\r  0%|          | 3/4289 [00:00<15:10,  4.71it/s]\\r  0%|          | 4/4289 [00:00<14:43,  4.85it/s]\\r  0%|          | 5/4289 [00:01<14:28,  4.93it/s]\\r  0%|          | 6/4289 [00:01<14:19,  4.98it/s]\\r  0%|          | 7/4289 [00:01<14:13,  5.02it/s]\\r  0%|          | 8/4289 [00:01<14:09,  5.04it/s]\\r  0%|          | 9/4289 [00:01<14:06,  5.06it/s]\\r  0%|          | 10/4289 [00:02<14:04,  5.07it/s]\\r  0%|          | 11/4289 [00:02<14:03,  5.07it/s]\\r  0%|          | 12/4289 [00:02<14:02,  5.08it/s]\\r  0%|          | 13/4289 [00:02<14:02,  5.07it/s]\\r  0%|          | 14/4289 [00:02<14:01,  5.08it/s]\\r  0%|          | 15/4289 [00:03<14:00,  5.08it/s]\\r  0%|          | 16/4289 [00:03<14:00,  5.08it/s]\\r  0%|          | 17/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 18/4289 [00:03<13:59,  5.08it/s]\\r  0%|          | 19/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 20/4289 [00:04<13:59,  5.09it/s]\\r  0%|          | 21/4289 [00:04<13:59,  5.09it/s]\\r  1%|          | 22/4289 [00:04<13:59,  5.08it/s]\\r  1%|          | 23/4289 [00:04<13:58,  5.09it/s]\\r  1%|          | 24/4289 [00:04<13:59,  5.08it/s]\\r  1%|          | 25/4289 [00:04<13:58,  5.08it/s]\\r  1%|          | 26/4289 [00:05<13:58,  5.09it/s]\\r  1%|          | 27/4289 [00:05<13:58,  5.08it/s]\\r  1%|          | 28/4289 [00:05<13:58,  5.08it/s]\\r  1%|          | 29/4289 [00:05<13:57,  5.09it/s]\\r  1%|          | 30/4289 [00:05<13:57,  5.09it/s]\\r  1%|          | 31/4289 [00:06<13:57,  5.09it/s]\\r  1%|          | 32/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 33/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 34/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 35/4289 [00:06<13:17,  5.33it/s]\\r  1%|          | 35/4289 [00:07<14:17,  4.96it/s]\\n2025-11-18 22:13:21.664 | INFO     | __main__:generate_long:491 - Generated 37 tokens in 8.32 seconds, 4.45 tokens/sec\\n2025-11-18 22:13:21.664 | INFO     | __main__:generate_long:494 - Bandwidth achieved: 3.83 GB/s\\n2025-11-18 22:13:21.664 | INFO     | __main__:generate_long:497 - GPU Memory used: 3.32 GB\\n2025-11-18 22:13:21.665 | INFO     | __main__:main:692 - Sampled text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n2025-11-18 22:13:21.672 | INFO     | __main__:main:697 - Saved codes to temp/codes_0.npy\\n2025-11-18 22:13:21.672 | INFO     | __main__:main:698 - Next sample\\n\")\n",
      "start process: 8\n",
      "python /nfs1/s1f102201582/projects/fish-speech/fish_speech/models/text2semantic/inference.py --text これはサンプルテキストです --prompt-text 元気に --prompt-tokens /nfs1/s1f102201582/projects/mhcc-moshi/moshi/fake.npy --output-dir ./temp --checkpoint-path /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/\n",
      "(b'', b\"2025-11-18 22:13:02.059 | INFO     | __main__:main:644 - Loading model ...\\n2025-11-18 22:13:02.289 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:432 - Loading model from /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini, config: DualARModelArgs(model_type='dual_ar', vocab_size=155776, n_layer=28, n_head=16, dim=1024, intermediate_size=3072, n_local_heads=8, head_dim=128, rope_base=1000000, norm_eps=1e-06, max_seq_len=8192, dropout=0.0, tie_word_embeddings=False, attention_qkv_bias=False, attention_o_bias=False, attention_qk_norm=True, codebook_size=4096, num_codebooks=10, use_gradient_checkpointing=True, initializer_range=0.03125, is_reward_model=False, scale_codebook_embeddings=True, n_fast_layer=4, fast_dim=1024, fast_n_head=16, fast_n_local_heads=8, fast_head_dim=64, fast_intermediate_size=3072, fast_attention_qkv_bias=False, fast_attention_qk_norm=False, fast_attention_o_bias=False)\\n2025-11-18 22:13:10.649 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:494 - Model weights loaded - Status: <All keys matched successfully>\\n2025-11-18 22:13:13.324 | INFO     | __main__:init_model:357 - Restored model from checkpoint\\n2025-11-18 22:13:13.324 | INFO     | __main__:init_model:362 - Using DualARTransformer\\n2025-11-18 22:13:13.348 | INFO     | __main__:main:658 - Time to load model: 11.29 seconds\\n2025-11-18 22:13:13.371 | INFO     | __main__:generate_long:457 - Encoded text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n\\r  0%|          | 0/4289 [00:00<?, ?it/s]\\r  0%|          | 1/4289 [00:00<17:28,  4.09it/s]\\r  0%|          | 2/4289 [00:00<15:39,  4.56it/s]\\r  0%|          | 3/4289 [00:00<14:54,  4.79it/s]\\r  0%|          | 4/4289 [00:00<14:33,  4.90it/s]\\r  0%|          | 5/4289 [00:01<14:22,  4.97it/s]\\r  0%|          | 6/4289 [00:01<14:15,  5.01it/s]\\r  0%|          | 7/4289 [00:01<14:10,  5.03it/s]\\r  0%|          | 8/4289 [00:01<14:07,  5.05it/s]\\r  0%|          | 9/4289 [00:01<14:04,  5.07it/s]\\r  0%|          | 10/4289 [00:02<14:03,  5.07it/s]\\r  0%|          | 11/4289 [00:02<14:02,  5.08it/s]\\r  0%|          | 12/4289 [00:02<14:01,  5.08it/s]\\r  0%|          | 13/4289 [00:02<14:02,  5.08it/s]\\r  0%|          | 14/4289 [00:02<14:01,  5.08it/s]\\r  0%|          | 15/4289 [00:03<14:00,  5.08it/s]\\r  0%|          | 16/4289 [00:03<14:00,  5.08it/s]\\r  0%|          | 17/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 18/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 19/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 20/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 21/4289 [00:04<13:59,  5.09it/s]\\r  1%|          | 22/4289 [00:04<13:59,  5.08it/s]\\r  1%|          | 23/4289 [00:04<13:58,  5.09it/s]\\r  1%|          | 24/4289 [00:04<13:59,  5.08it/s]\\r  1%|          | 25/4289 [00:04<13:58,  5.08it/s]\\r  1%|          | 26/4289 [00:05<13:58,  5.09it/s]\\r  1%|          | 27/4289 [00:05<13:58,  5.08it/s]\\r  1%|          | 28/4289 [00:05<13:58,  5.08it/s]\\r  1%|          | 29/4289 [00:05<13:57,  5.09it/s]\\r  1%|          | 30/4289 [00:05<13:57,  5.09it/s]\\r  1%|          | 31/4289 [00:06<13:57,  5.09it/s]\\r  1%|          | 32/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 33/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 34/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 35/4289 [00:06<13:17,  5.33it/s]\\r  1%|          | 35/4289 [00:07<14:15,  4.97it/s]\\n2025-11-18 22:13:21.664 | INFO     | __main__:generate_long:491 - Generated 37 tokens in 8.29 seconds, 4.46 tokens/sec\\n2025-11-18 22:13:21.664 | INFO     | __main__:generate_long:494 - Bandwidth achieved: 3.84 GB/s\\n2025-11-18 22:13:21.664 | INFO     | __main__:generate_long:497 - GPU Memory used: 3.32 GB\\n2025-11-18 22:13:21.665 | INFO     | __main__:main:692 - Sampled text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n2025-11-18 22:13:21.672 | INFO     | __main__:main:697 - Saved codes to temp/codes_0.npy\\n2025-11-18 22:13:21.672 | INFO     | __main__:main:698 - Next sample\\n\")\n",
      "start process: 9\n",
      "python /nfs1/s1f102201582/projects/fish-speech/fish_speech/models/text2semantic/inference.py --text これはサンプルテキストです --prompt-text 元気に --prompt-tokens /nfs1/s1f102201582/projects/mhcc-moshi/moshi/fake.npy --output-dir ./temp --checkpoint-path /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/\n",
      "(b'', b\"2025-11-18 22:13:02.059 | INFO     | __main__:main:644 - Loading model ...\\n2025-11-18 22:13:02.289 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:432 - Loading model from /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini, config: DualARModelArgs(model_type='dual_ar', vocab_size=155776, n_layer=28, n_head=16, dim=1024, intermediate_size=3072, n_local_heads=8, head_dim=128, rope_base=1000000, norm_eps=1e-06, max_seq_len=8192, dropout=0.0, tie_word_embeddings=False, attention_qkv_bias=False, attention_o_bias=False, attention_qk_norm=True, codebook_size=4096, num_codebooks=10, use_gradient_checkpointing=True, initializer_range=0.03125, is_reward_model=False, scale_codebook_embeddings=True, n_fast_layer=4, fast_dim=1024, fast_n_head=16, fast_n_local_heads=8, fast_head_dim=64, fast_intermediate_size=3072, fast_attention_qkv_bias=False, fast_attention_qk_norm=False, fast_attention_o_bias=False)\\n2025-11-18 22:13:10.656 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:494 - Model weights loaded - Status: <All keys matched successfully>\\n2025-11-18 22:13:13.265 | INFO     | __main__:init_model:357 - Restored model from checkpoint\\n2025-11-18 22:13:13.265 | INFO     | __main__:init_model:362 - Using DualARTransformer\\n2025-11-18 22:13:13.281 | INFO     | __main__:main:658 - Time to load model: 11.22 seconds\\n2025-11-18 22:13:13.304 | INFO     | __main__:generate_long:457 - Encoded text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n\\r  0%|          | 0/4289 [00:00<?, ?it/s]\\r  0%|          | 1/4289 [00:00<18:28,  3.87it/s]\\r  0%|          | 2/4289 [00:00<15:40,  4.56it/s]\\r  0%|          | 3/4289 [00:00<14:55,  4.79it/s]\\r  0%|          | 4/4289 [00:00<14:33,  4.90it/s]\\r  0%|          | 5/4289 [00:01<14:22,  4.97it/s]\\r  0%|          | 6/4289 [00:01<14:15,  5.01it/s]\\r  0%|          | 7/4289 [00:01<14:10,  5.03it/s]\\r  0%|          | 8/4289 [00:01<14:07,  5.05it/s]\\r  0%|          | 9/4289 [00:01<14:04,  5.07it/s]\\r  0%|          | 10/4289 [00:02<14:03,  5.07it/s]\\r  0%|          | 11/4289 [00:02<14:02,  5.08it/s]\\r  0%|          | 12/4289 [00:02<14:01,  5.08it/s]\\r  0%|          | 13/4289 [00:02<14:02,  5.08it/s]\\r  0%|          | 14/4289 [00:02<14:01,  5.08it/s]\\r  0%|          | 15/4289 [00:03<14:00,  5.08it/s]\\r  0%|          | 16/4289 [00:03<14:00,  5.09it/s]\\r  0%|          | 17/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 18/4289 [00:03<14:00,  5.08it/s]\\r  0%|          | 19/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 20/4289 [00:03<13:59,  5.09it/s]\\r  0%|          | 21/4289 [00:04<13:59,  5.09it/s]\\r  1%|          | 22/4289 [00:04<13:59,  5.08it/s]\\r  1%|          | 23/4289 [00:04<13:58,  5.09it/s]\\r  1%|          | 24/4289 [00:04<13:59,  5.08it/s]\\r  1%|          | 25/4289 [00:04<13:58,  5.08it/s]\\r  1%|          | 26/4289 [00:05<13:58,  5.09it/s]\\r  1%|          | 27/4289 [00:05<13:58,  5.08it/s]\\r  1%|          | 28/4289 [00:05<13:58,  5.08it/s]\\r  1%|          | 29/4289 [00:05<13:57,  5.09it/s]\\r  1%|          | 30/4289 [00:05<13:57,  5.09it/s]\\r  1%|          | 31/4289 [00:06<13:57,  5.09it/s]\\r  1%|          | 32/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 33/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 34/4289 [00:06<13:56,  5.09it/s]\\r  1%|          | 35/4289 [00:06<13:23,  5.30it/s]\\r  1%|          | 35/4289 [00:07<14:18,  4.95it/s]\\n2025-11-18 22:13:21.645 | INFO     | __main__:generate_long:491 - Generated 37 tokens in 8.34 seconds, 4.44 tokens/sec\\n2025-11-18 22:13:21.645 | INFO     | __main__:generate_long:494 - Bandwidth achieved: 3.82 GB/s\\n2025-11-18 22:13:21.646 | INFO     | __main__:generate_long:497 - GPU Memory used: 3.32 GB\\n2025-11-18 22:13:21.648 | INFO     | __main__:main:692 - Sampled text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n2025-11-18 22:13:21.671 | INFO     | __main__:main:697 - Saved codes to temp/codes_0.npy\\n2025-11-18 22:13:21.671 | INFO     | __main__:main:698 - Next sample\\n\")\n",
      "(b'', b\"2025-11-18 22:13:27.126 | INFO     | __main__:main:644 - Loading model ...\\n2025-11-18 22:13:27.355 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:432 - Loading model from /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini, config: DualARModelArgs(model_type='dual_ar', vocab_size=155776, n_layer=28, n_head=16, dim=1024, intermediate_size=3072, n_local_heads=8, head_dim=128, rope_base=1000000, norm_eps=1e-06, max_seq_len=8192, dropout=0.0, tie_word_embeddings=False, attention_qkv_bias=False, attention_o_bias=False, attention_qk_norm=True, codebook_size=4096, num_codebooks=10, use_gradient_checkpointing=True, initializer_range=0.03125, is_reward_model=False, scale_codebook_embeddings=True, n_fast_layer=4, fast_dim=1024, fast_n_head=16, fast_n_local_heads=8, fast_head_dim=64, fast_intermediate_size=3072, fast_attention_qkv_bias=False, fast_attention_qk_norm=False, fast_attention_o_bias=False)\\n2025-11-18 22:13:35.618 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:494 - Model weights loaded - Status: <All keys matched successfully>\\n2025-11-18 22:13:38.144 | INFO     | __main__:init_model:357 - Restored model from checkpoint\\n2025-11-18 22:13:38.144 | INFO     | __main__:init_model:362 - Using DualARTransformer\\n2025-11-18 22:13:38.163 | INFO     | __main__:main:658 - Time to load model: 11.04 seconds\\n2025-11-18 22:13:38.186 | INFO     | __main__:generate_long:457 - Encoded text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n\\r  0%|          | 0/4289 [00:00<?, ?it/s]\\r  0%|          | 1/4289 [00:00<26:26,  2.70it/s]\\r  0%|          | 2/4289 [00:00<15:43,  4.54it/s]\\r  0%|          | 3/4289 [00:00<12:15,  5.83it/s]\\r  0%|          | 4/4289 [00:00<11:35,  6.16it/s]\\r  0%|          | 5/4289 [00:00<12:31,  5.70it/s]\\r  0%|          | 6/4289 [00:01<12:56,  5.52it/s]\\r  0%|          | 7/4289 [00:01<13:16,  5.38it/s]\\r  0%|          | 8/4289 [00:01<13:29,  5.29it/s]\\r  0%|          | 9/4289 [00:01<13:38,  5.23it/s]\\r  0%|          | 10/4289 [00:01<13:46,  5.18it/s]\\r  0%|          | 11/4289 [00:02<13:49,  5.16it/s]\\r  0%|          | 12/4289 [00:02<13:52,  5.14it/s]\\r  0%|          | 13/4289 [00:02<13:53,  5.13it/s]\\r  0%|          | 14/4289 [00:02<13:53,  5.13it/s]\\r  0%|          | 15/4289 [00:02<13:54,  5.12it/s]\\r  0%|          | 16/4289 [00:03<13:55,  5.12it/s]\\r  0%|          | 17/4289 [00:03<13:56,  5.11it/s]\\r  0%|          | 18/4289 [00:03<13:55,  5.11it/s]\\r  0%|          | 19/4289 [00:03<13:55,  5.11it/s]\\r  0%|          | 20/4289 [00:03<13:54,  5.11it/s]\\r  0%|          | 21/4289 [00:04<13:54,  5.11it/s]\\r  1%|          | 22/4289 [00:04<13:54,  5.11it/s]\\r  1%|          | 23/4289 [00:04<13:56,  5.10it/s]\\r  1%|          | 24/4289 [00:04<13:56,  5.10it/s]\\r  1%|          | 25/4289 [00:04<13:55,  5.10it/s]\\r  1%|          | 26/4289 [00:05<13:53,  5.11it/s]\\r  1%|          | 27/4289 [00:05<13:54,  5.11it/s]\\r  1%|          | 28/4289 [00:05<13:54,  5.11it/s]\\r  1%|          | 29/4289 [00:05<13:54,  5.11it/s]\\r  1%|          | 30/4289 [00:05<13:54,  5.11it/s]\\r  1%|          | 31/4289 [00:06<13:52,  5.11it/s]\\r  1%|          | 32/4289 [00:06<13:52,  5.11it/s]\\r  1%|          | 33/4289 [00:06<13:52,  5.11it/s]\\r  1%|          | 34/4289 [00:06<13:54,  5.10it/s]\\r  1%|          | 35/4289 [00:06<13:53,  5.11it/s]\\r  1%|          | 35/4289 [00:07<14:12,  4.99it/s]\\n2025-11-18 22:13:46.106 | INFO     | __main__:generate_long:491 - Generated 37 tokens in 7.92 seconds, 4.67 tokens/sec\\n2025-11-18 22:13:46.106 | INFO     | __main__:generate_long:494 - Bandwidth achieved: 4.02 GB/s\\n2025-11-18 22:13:46.106 | INFO     | __main__:generate_long:497 - GPU Memory used: 3.32 GB\\n2025-11-18 22:13:46.121 | INFO     | __main__:main:692 - Sampled text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n2025-11-18 22:13:46.150 | INFO     | __main__:main:697 - Saved codes to temp/codes_0.npy\\n2025-11-18 22:13:46.150 | INFO     | __main__:main:698 - Next sample\\n\")\n",
      "(b'', b\"2025-11-18 22:13:27.410 | INFO     | __main__:main:644 - Loading model ...\\n2025-11-18 22:13:27.643 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:432 - Loading model from /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini, config: DualARModelArgs(model_type='dual_ar', vocab_size=155776, n_layer=28, n_head=16, dim=1024, intermediate_size=3072, n_local_heads=8, head_dim=128, rope_base=1000000, norm_eps=1e-06, max_seq_len=8192, dropout=0.0, tie_word_embeddings=False, attention_qkv_bias=False, attention_o_bias=False, attention_qk_norm=True, codebook_size=4096, num_codebooks=10, use_gradient_checkpointing=True, initializer_range=0.03125, is_reward_model=False, scale_codebook_embeddings=True, n_fast_layer=4, fast_dim=1024, fast_n_head=16, fast_n_local_heads=8, fast_head_dim=64, fast_intermediate_size=3072, fast_attention_qkv_bias=False, fast_attention_qk_norm=False, fast_attention_o_bias=False)\\n2025-11-18 22:13:35.943 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:494 - Model weights loaded - Status: <All keys matched successfully>\\n2025-11-18 22:13:38.303 | INFO     | __main__:init_model:357 - Restored model from checkpoint\\n2025-11-18 22:13:38.303 | INFO     | __main__:init_model:362 - Using DualARTransformer\\n2025-11-18 22:13:38.321 | INFO     | __main__:main:658 - Time to load model: 10.91 seconds\\n2025-11-18 22:13:38.344 | INFO     | __main__:generate_long:457 - Encoded text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n\\r  0%|          | 0/4289 [00:00<?, ?it/s]\\r  0%|          | 1/4289 [00:00<16:16,  4.39it/s]\\r  0%|          | 2/4289 [00:00<14:35,  4.89it/s]\\r  0%|          | 3/4289 [00:00<14:11,  5.03it/s]\\r  0%|          | 4/4289 [00:00<14:07,  5.06it/s]\\r  0%|          | 5/4289 [00:00<14:03,  5.08it/s]\\r  0%|          | 6/4289 [00:01<14:01,  5.09it/s]\\r  0%|          | 7/4289 [00:01<14:02,  5.08it/s]\\r  0%|          | 8/4289 [00:01<14:00,  5.09it/s]\\r  0%|          | 9/4289 [00:01<14:00,  5.09it/s]\\r  0%|          | 10/4289 [00:01<13:59,  5.10it/s]\\r  0%|          | 11/4289 [00:02<13:58,  5.10it/s]\\r  0%|          | 12/4289 [00:02<13:57,  5.11it/s]\\r  0%|          | 13/4289 [00:02<13:57,  5.11it/s]\\r  0%|          | 14/4289 [00:02<13:58,  5.10it/s]\\r  0%|          | 15/4289 [00:02<13:57,  5.10it/s]\\r  0%|          | 16/4289 [00:03<13:56,  5.11it/s]\\r  0%|          | 17/4289 [00:03<13:55,  5.11it/s]\\r  0%|          | 18/4289 [00:03<13:55,  5.11it/s]\\r  0%|          | 19/4289 [00:03<13:55,  5.11it/s]\\r  0%|          | 20/4289 [00:03<13:56,  5.10it/s]\\r  0%|          | 21/4289 [00:04<13:57,  5.10it/s]\\r  1%|          | 22/4289 [00:04<13:56,  5.10it/s]\\r  1%|          | 23/4289 [00:04<13:54,  5.11it/s]\\r  1%|          | 24/4289 [00:04<13:54,  5.11it/s]\\r  1%|          | 25/4289 [00:04<13:54,  5.11it/s]\\r  1%|          | 26/4289 [00:05<13:54,  5.11it/s]\\r  1%|          | 27/4289 [00:05<13:54,  5.11it/s]\\r  1%|          | 28/4289 [00:05<13:53,  5.11it/s]\\r  1%|          | 29/4289 [00:05<13:53,  5.11it/s]\\r  1%|          | 30/4289 [00:05<13:52,  5.11it/s]\\r  1%|          | 31/4289 [00:06<13:54,  5.10it/s]\\r  1%|          | 32/4289 [00:06<13:53,  5.10it/s]\\r  1%|          | 33/4289 [00:06<13:52,  5.11it/s]\\r  1%|          | 34/4289 [00:06<13:13,  5.36it/s]\\r  1%|          | 35/4289 [00:06<12:36,  5.63it/s]\\r  1%|          | 35/4289 [00:06<14:06,  5.03it/s]\\n2025-11-18 22:13:46.534 | INFO     | __main__:generate_long:491 - Generated 37 tokens in 8.19 seconds, 4.52 tokens/sec\\n2025-11-18 22:13:46.534 | INFO     | __main__:generate_long:494 - Bandwidth achieved: 3.89 GB/s\\n2025-11-18 22:13:46.534 | INFO     | __main__:generate_long:497 - GPU Memory used: 3.32 GB\\n2025-11-18 22:13:46.539 | INFO     | __main__:main:692 - Sampled text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n2025-11-18 22:13:46.541 | INFO     | __main__:main:697 - Saved codes to temp/codes_0.npy\\n2025-11-18 22:13:46.541 | INFO     | __main__:main:698 - Next sample\\n\")\n",
      "(b'', b\"2025-11-18 22:13:27.478 | INFO     | __main__:main:644 - Loading model ...\\n2025-11-18 22:13:27.713 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:432 - Loading model from /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini, config: DualARModelArgs(model_type='dual_ar', vocab_size=155776, n_layer=28, n_head=16, dim=1024, intermediate_size=3072, n_local_heads=8, head_dim=128, rope_base=1000000, norm_eps=1e-06, max_seq_len=8192, dropout=0.0, tie_word_embeddings=False, attention_qkv_bias=False, attention_o_bias=False, attention_qk_norm=True, codebook_size=4096, num_codebooks=10, use_gradient_checkpointing=True, initializer_range=0.03125, is_reward_model=False, scale_codebook_embeddings=True, n_fast_layer=4, fast_dim=1024, fast_n_head=16, fast_n_local_heads=8, fast_head_dim=64, fast_intermediate_size=3072, fast_attention_qkv_bias=False, fast_attention_qk_norm=False, fast_attention_o_bias=False)\\n2025-11-18 22:13:35.992 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:494 - Model weights loaded - Status: <All keys matched successfully>\\n2025-11-18 22:13:38.326 | INFO     | __main__:init_model:357 - Restored model from checkpoint\\n2025-11-18 22:13:38.326 | INFO     | __main__:init_model:362 - Using DualARTransformer\\n2025-11-18 22:13:38.351 | INFO     | __main__:main:658 - Time to load model: 10.87 seconds\\n2025-11-18 22:13:38.374 | INFO     | __main__:generate_long:457 - Encoded text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n\\r  0%|          | 0/4289 [00:00<?, ?it/s]\\r  0%|          | 1/4289 [00:00<16:29,  4.33it/s]\\r  0%|          | 2/4289 [00:00<14:52,  4.80it/s]\\r  0%|          | 3/4289 [00:00<14:21,  4.98it/s]\\r  0%|          | 4/4289 [00:00<14:12,  5.03it/s]\\r  0%|          | 5/4289 [00:01<14:06,  5.06it/s]\\r  0%|          | 6/4289 [00:01<14:03,  5.08it/s]\\r  0%|          | 7/4289 [00:01<14:04,  5.07it/s]\\r  0%|          | 8/4289 [00:01<14:01,  5.09it/s]\\r  0%|          | 9/4289 [00:01<14:01,  5.08it/s]\\r  0%|          | 10/4289 [00:01<13:59,  5.10it/s]\\r  0%|          | 11/4289 [00:02<13:58,  5.10it/s]\\r  0%|          | 12/4289 [00:02<13:57,  5.11it/s]\\r  0%|          | 13/4289 [00:02<13:57,  5.11it/s]\\r  0%|          | 14/4289 [00:02<13:58,  5.10it/s]\\r  0%|          | 15/4289 [00:02<13:57,  5.10it/s]\\r  0%|          | 16/4289 [00:03<13:56,  5.11it/s]\\r  0%|          | 17/4289 [00:03<13:55,  5.11it/s]\\r  0%|          | 18/4289 [00:03<13:55,  5.11it/s]\\r  0%|          | 19/4289 [00:03<13:55,  5.11it/s]\\r  0%|          | 20/4289 [00:03<13:57,  5.10it/s]\\r  0%|          | 21/4289 [00:04<13:56,  5.10it/s]\\r  1%|          | 22/4289 [00:04<13:56,  5.10it/s]\\r  1%|          | 23/4289 [00:04<13:54,  5.11it/s]\\r  1%|          | 24/4289 [00:04<13:54,  5.11it/s]\\r  1%|          | 25/4289 [00:04<13:54,  5.11it/s]\\r  1%|          | 26/4289 [00:05<13:54,  5.11it/s]\\r  1%|          | 27/4289 [00:05<13:54,  5.11it/s]\\r  1%|          | 28/4289 [00:05<13:53,  5.11it/s]\\r  1%|          | 29/4289 [00:05<13:53,  5.11it/s]\\r  1%|          | 30/4289 [00:05<13:53,  5.11it/s]\\r  1%|          | 31/4289 [00:06<13:55,  5.10it/s]\\r  1%|          | 32/4289 [00:06<13:53,  5.11it/s]\\r  1%|          | 33/4289 [00:06<13:52,  5.11it/s]\\r  1%|          | 34/4289 [00:06<13:08,  5.40it/s]\\r  1%|          | 35/4289 [00:06<12:31,  5.66it/s]\\r  1%|          | 35/4289 [00:06<14:05,  5.03it/s]\\n2025-11-18 22:13:46.550 | INFO     | __main__:generate_long:491 - Generated 37 tokens in 8.18 seconds, 4.53 tokens/sec\\n2025-11-18 22:13:46.550 | INFO     | __main__:generate_long:494 - Bandwidth achieved: 3.89 GB/s\\n2025-11-18 22:13:46.550 | INFO     | __main__:generate_long:497 - GPU Memory used: 3.32 GB\\n2025-11-18 22:13:46.553 | INFO     | __main__:main:692 - Sampled text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n2025-11-18 22:13:46.565 | INFO     | __main__:main:697 - Saved codes to temp/codes_0.npy\\n2025-11-18 22:13:46.565 | INFO     | __main__:main:698 - Next sample\\n\")\n",
      "(b'', b\"2025-11-18 22:13:27.556 | INFO     | __main__:main:644 - Loading model ...\\n2025-11-18 22:13:27.789 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:432 - Loading model from /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini, config: DualARModelArgs(model_type='dual_ar', vocab_size=155776, n_layer=28, n_head=16, dim=1024, intermediate_size=3072, n_local_heads=8, head_dim=128, rope_base=1000000, norm_eps=1e-06, max_seq_len=8192, dropout=0.0, tie_word_embeddings=False, attention_qkv_bias=False, attention_o_bias=False, attention_qk_norm=True, codebook_size=4096, num_codebooks=10, use_gradient_checkpointing=True, initializer_range=0.03125, is_reward_model=False, scale_codebook_embeddings=True, n_fast_layer=4, fast_dim=1024, fast_n_head=16, fast_n_local_heads=8, fast_head_dim=64, fast_intermediate_size=3072, fast_attention_qkv_bias=False, fast_attention_qk_norm=False, fast_attention_o_bias=False)\\n2025-11-18 22:13:36.057 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:494 - Model weights loaded - Status: <All keys matched successfully>\\n2025-11-18 22:13:38.350 | INFO     | __main__:init_model:357 - Restored model from checkpoint\\n2025-11-18 22:13:38.350 | INFO     | __main__:init_model:362 - Using DualARTransformer\\n2025-11-18 22:13:38.368 | INFO     | __main__:main:658 - Time to load model: 10.81 seconds\\n2025-11-18 22:13:38.398 | INFO     | __main__:generate_long:457 - Encoded text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n\\r  0%|          | 0/4289 [00:00<?, ?it/s]\\r  0%|          | 1/4289 [00:00<17:50,  4.00it/s]\\r  0%|          | 2/4289 [00:00<15:12,  4.70it/s]\\r  0%|          | 3/4289 [00:00<15:05,  4.73it/s]\\r  0%|          | 4/4289 [00:00<14:38,  4.87it/s]\\r  0%|          | 5/4289 [00:01<14:23,  4.96it/s]\\r  0%|          | 6/4289 [00:01<14:14,  5.01it/s]\\r  0%|          | 7/4289 [00:01<14:11,  5.03it/s]\\r  0%|          | 8/4289 [00:01<14:05,  5.06it/s]\\r  0%|          | 9/4289 [00:01<14:05,  5.06it/s]\\r  0%|          | 10/4289 [00:02<14:02,  5.08it/s]\\r  0%|          | 11/4289 [00:02<14:00,  5.09it/s]\\r  0%|          | 12/4289 [00:02<13:59,  5.10it/s]\\r  0%|          | 13/4289 [00:02<13:57,  5.10it/s]\\r  0%|          | 14/4289 [00:02<13:59,  5.09it/s]\\r  0%|          | 15/4289 [00:02<13:58,  5.10it/s]\\r  0%|          | 16/4289 [00:03<13:56,  5.11it/s]\\r  0%|          | 17/4289 [00:03<13:56,  5.11it/s]\\r  0%|          | 18/4289 [00:03<13:55,  5.11it/s]\\r  0%|          | 19/4289 [00:03<13:54,  5.12it/s]\\r  0%|          | 20/4289 [00:03<13:57,  5.10it/s]\\r  0%|          | 21/4289 [00:04<13:56,  5.10it/s]\\r  1%|          | 22/4289 [00:04<13:55,  5.10it/s]\\r  1%|          | 23/4289 [00:04<13:54,  5.11it/s]\\r  1%|          | 24/4289 [00:04<13:55,  5.11it/s]\\r  1%|          | 25/4289 [00:04<13:55,  5.11it/s]\\r  1%|          | 26/4289 [00:05<13:54,  5.11it/s]\\r  1%|          | 27/4289 [00:05<13:54,  5.11it/s]\\r  1%|          | 28/4289 [00:05<13:53,  5.11it/s]\\r  1%|          | 29/4289 [00:05<13:53,  5.11it/s]\\r  1%|          | 30/4289 [00:05<13:52,  5.12it/s]\\r  1%|          | 31/4289 [00:06<13:55,  5.10it/s]\\r  1%|          | 32/4289 [00:06<13:53,  5.11it/s]\\r  1%|          | 33/4289 [00:06<13:32,  5.24it/s]\\r  1%|          | 34/4289 [00:06<12:51,  5.51it/s]\\r  1%|          | 35/4289 [00:06<12:20,  5.75it/s]\\r  1%|          | 35/4289 [00:06<14:03,  5.04it/s]\\n2025-11-18 22:13:46.580 | INFO     | __main__:generate_long:491 - Generated 37 tokens in 8.18 seconds, 4.52 tokens/sec\\n2025-11-18 22:13:46.580 | INFO     | __main__:generate_long:494 - Bandwidth achieved: 3.89 GB/s\\n2025-11-18 22:13:46.581 | INFO     | __main__:generate_long:497 - GPU Memory used: 3.32 GB\\n2025-11-18 22:13:46.586 | INFO     | __main__:main:692 - Sampled text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n2025-11-18 22:13:46.588 | INFO     | __main__:main:697 - Saved codes to temp/codes_0.npy\\n2025-11-18 22:13:46.588 | INFO     | __main__:main:698 - Next sample\\n\")\n",
      "(b'', b\"2025-11-18 22:13:28.395 | INFO     | __main__:main:644 - Loading model ...\\n2025-11-18 22:13:28.623 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:432 - Loading model from /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini, config: DualARModelArgs(model_type='dual_ar', vocab_size=155776, n_layer=28, n_head=16, dim=1024, intermediate_size=3072, n_local_heads=8, head_dim=128, rope_base=1000000, norm_eps=1e-06, max_seq_len=8192, dropout=0.0, tie_word_embeddings=False, attention_qkv_bias=False, attention_o_bias=False, attention_qk_norm=True, codebook_size=4096, num_codebooks=10, use_gradient_checkpointing=True, initializer_range=0.03125, is_reward_model=False, scale_codebook_embeddings=True, n_fast_layer=4, fast_dim=1024, fast_n_head=16, fast_n_local_heads=8, fast_head_dim=64, fast_intermediate_size=3072, fast_attention_qkv_bias=False, fast_attention_qk_norm=False, fast_attention_o_bias=False)\\n2025-11-18 22:13:36.825 | INFO     | fish_speech.models.text2semantic.llama:from_pretrained:494 - Model weights loaded - Status: <All keys matched successfully>\\n2025-11-18 22:13:38.496 | INFO     | __main__:init_model:357 - Restored model from checkpoint\\n2025-11-18 22:13:38.496 | INFO     | __main__:init_model:362 - Using DualARTransformer\\n2025-11-18 22:13:38.515 | INFO     | __main__:main:658 - Time to load model: 10.12 seconds\\n2025-11-18 22:13:38.538 | INFO     | __main__:generate_long:457 - Encoded text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n\\r  0%|          | 0/4289 [00:00<?, ?it/s]\\r  0%|          | 1/4289 [00:00<19:43,  3.62it/s]\\r  0%|          | 2/4289 [00:00<16:32,  4.32it/s]\\r  0%|          | 3/4289 [00:00<15:22,  4.65it/s]\\r  0%|          | 4/4289 [00:00<14:48,  4.82it/s]\\r  0%|          | 5/4289 [00:01<14:31,  4.92it/s]\\r  0%|          | 6/4289 [00:01<14:21,  4.97it/s]\\r  0%|          | 7/4289 [00:01<14:13,  5.02it/s]\\r  0%|          | 8/4289 [00:01<14:09,  5.04it/s]\\r  0%|          | 9/4289 [00:01<14:05,  5.06it/s]\\r  0%|          | 10/4289 [00:02<14:02,  5.08it/s]\\r  0%|          | 11/4289 [00:02<14:00,  5.09it/s]\\r  0%|          | 12/4289 [00:02<13:59,  5.09it/s]\\r  0%|          | 13/4289 [00:02<13:59,  5.09it/s]\\r  0%|          | 14/4289 [00:02<13:58,  5.10it/s]\\r  0%|          | 15/4289 [00:03<13:57,  5.10it/s]\\r  0%|          | 16/4289 [00:03<13:56,  5.11it/s]\\r  0%|          | 17/4289 [00:03<13:56,  5.11it/s]\\r  0%|          | 18/4289 [00:03<13:56,  5.11it/s]\\r  0%|          | 19/4289 [00:03<13:57,  5.10it/s]\\r  0%|          | 20/4289 [00:04<13:57,  5.10it/s]\\r  0%|          | 21/4289 [00:04<13:56,  5.10it/s]\\r  1%|          | 22/4289 [00:04<13:54,  5.11it/s]\\r  1%|          | 23/4289 [00:04<13:55,  5.11it/s]\\r  1%|          | 24/4289 [00:04<13:55,  5.11it/s]\\r  1%|          | 25/4289 [00:04<13:54,  5.11it/s]\\r  1%|          | 26/4289 [00:05<13:54,  5.11it/s]\\r  1%|          | 27/4289 [00:05<13:53,  5.11it/s]\\r  1%|          | 28/4289 [00:05<13:53,  5.11it/s]\\r  1%|          | 29/4289 [00:05<13:53,  5.11it/s]\\r  1%|          | 30/4289 [00:05<13:55,  5.10it/s]\\r  1%|          | 31/4289 [00:06<13:53,  5.11it/s]\\r  1%|          | 32/4289 [00:06<13:44,  5.16it/s]\\r  1%|          | 33/4289 [00:06<12:59,  5.46it/s]\\r  1%|          | 34/4289 [00:06<12:26,  5.70it/s]\\r  1%|          | 35/4289 [00:06<11:24,  6.22it/s]\\r  1%|          | 35/4289 [00:06<13:53,  5.10it/s]\\n2025-11-18 22:13:46.633 | INFO     | __main__:generate_long:491 - Generated 37 tokens in 8.10 seconds, 4.57 tokens/sec\\n2025-11-18 22:13:46.633 | INFO     | __main__:generate_long:494 - Bandwidth achieved: 3.93 GB/s\\n2025-11-18 22:13:46.633 | INFO     | __main__:generate_long:497 - GPU Memory used: 3.32 GB\\n2025-11-18 22:13:46.634 | INFO     | __main__:main:692 - Sampled text: \\xe3\\x81\\x93\\xe3\\x82\\x8c\\xe3\\x81\\xaf\\xe3\\x82\\xb5\\xe3\\x83\\xb3\\xe3\\x83\\x97\\xe3\\x83\\xab\\xe3\\x83\\x86\\xe3\\x82\\xad\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x81\\xa7\\xe3\\x81\\x99\\n2025-11-18 22:13:46.635 | INFO     | __main__:main:697 - Saved codes to temp/codes_0.npy\\n2025-11-18 22:13:46.635 | INFO     | __main__:main:698 - Next sample\\n\")\n",
      "[22:13:48] asyncio.run() が終了しました (実行時間: 51.59s)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONAYNCIODEBUG\"] = \"1\"\n",
    "\n",
    "print(f\"[{time.strftime('%X')}] asyncio.run(main()) を呼び出します\")\n",
    "start_run = time.time()\n",
    "await main()\n",
    "end_run = time.time()\n",
    "print(f\"[{time.strftime('%X')}] asyncio.run() が終了しました (実行時間: {end_run - start_run:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a82ca8-d0d8-4537-b259-8e618d1448ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
