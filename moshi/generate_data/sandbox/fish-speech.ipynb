{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "688f3a23-3dcd-44ce-abcd-7d86351a0e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/nfs1/s1f102201582/projects/amitaro/2023-09-12_ChantsofSennaar_01.wav' を読み込んでいます...\n",
      "読み込み完了。サンプリングレート: 44100 Hz, 長さ: 180.00 秒\n",
      "'out.wav' に保存しています...\n",
      "保存が完了しました。\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "def cut_and_save_audio(input_path, output_path, duration_minutes=30):\n",
    "    \"\"\"\n",
    "    音声を読み込み、指定した時間（分）で切り取って別ファイルに保存する\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): 入力する音声ファイルのパス\n",
    "        output_path (str): 保存する音声ファイルのパス\n",
    "        duration_minutes (int): 切り取る時間（分）\n",
    "    \"\"\"\n",
    "    \n",
    "    # 分を秒に変換\n",
    "    duration_seconds = duration_minutes * 60\n",
    "    \n",
    "    print(f\"'{input_path}' を読み込んでいます...\")\n",
    "    \n",
    "    try:\n",
    "        # sr=None で元のサンプリングレートを維持し、durationで読み込む長さを指定\n",
    "        # これが最も効率的な方法です\n",
    "        y, sr = librosa.load(\n",
    "            input_path, \n",
    "            sr=None,  # 元のサンプリングレートを維持\n",
    "            duration=duration_seconds  # 先頭から指定秒数だけ読み込む\n",
    "        )\n",
    "        \n",
    "        print(f\"読み込み完了。サンプリングレート: {sr} Hz, 長さ: {len(y)/sr:.2f} 秒\")\n",
    "\n",
    "        # soundfile を使って保存\n",
    "        print(f\"'{output_path}' に保存しています...\")\n",
    "        sf.write(output_path, y, sr)\n",
    "        \n",
    "        print(f\"保存が完了しました。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# 1. 入力ファイルと出力ファイルを指定してください\n",
    "# (例: \"long_meeting_audio.wav\")\n",
    "INPUT_FILE = \"/nfs1/s1f102201582/projects/amitaro/2023-09-12_ChantsofSennaar_01.wav\" \n",
    "OUTPUT_FILE = \"out.wav\" \n",
    "\n",
    "# ファイルが存在するかチェック\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"エラー: 入力ファイル '{INPUT_FILE}' が見つかりません。\")\n",
    "    print(\"INPUT_FILE のパスを正しく設定してください。\")\n",
    "else:\n",
    "    # 関数を実行\n",
    "    cut_and_save_audio(INPUT_FILE, OUTPUT_FILE, duration_minutes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a475af85-cf33-46ea-bc85-36f02e66663e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs1/s1f102201582/anaconda3/envs/fish/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "\u001b[32m2025-11-10 23:27:10.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mLoaded model: <All keys matched successfully>\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:10.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mProcessing in-place reconstruction of out.wav\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:11.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mLoaded audio with 180.00 seconds\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:12.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mGenerated indices of shape torch.Size([10, 3876])\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:13.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mGenerated audio of shape torch.Size([1, 1, 7938048]), equivalent to 180.00 seconds from 3876 features, features/second: 21.53\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:13.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mSaved audio to fake.wav\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python /users/s1f102201582/projects/fish-speech/fish_speech/models/dac/inference.py \\\n",
    "    -i \"out.wav\" \\\n",
    "    --checkpoint-path \"/nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/codec.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60761464-eb9a-4881-ad16-2679933ead4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-10 23:27:23.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m644\u001b[0m - \u001b[1mLoading model ...\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:23.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.llama\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m432\u001b[0m - \u001b[1mLoading model from /nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini, config: DualARModelArgs(model_type='dual_ar', vocab_size=155776, n_layer=28, n_head=16, dim=1024, intermediate_size=3072, n_local_heads=8, head_dim=128, rope_base=1000000, norm_eps=1e-06, max_seq_len=8192, dropout=0.0, tie_word_embeddings=False, attention_qkv_bias=False, attention_o_bias=False, attention_qk_norm=True, codebook_size=4096, num_codebooks=10, use_gradient_checkpointing=True, initializer_range=0.03125, is_reward_model=False, scale_codebook_embeddings=True, n_fast_layer=4, fast_dim=1024, fast_n_head=16, fast_n_local_heads=8, fast_head_dim=64, fast_intermediate_size=3072, fast_attention_qkv_bias=False, fast_attention_qk_norm=False, fast_attention_o_bias=False)\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:32.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.llama\u001b[0m:\u001b[36mfrom_pretrained\u001b[0m:\u001b[36m494\u001b[0m - \u001b[1mModel weights loaded - Status: <All keys matched successfully>\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:33.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_model\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mRestored model from checkpoint\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:33.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_model\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mUsing DualARTransformer\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:33.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minit_model\u001b[0m:\u001b[36m375\u001b[0m - \u001b[1mCompiling function...\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:34.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m658\u001b[0m - \u001b[1mTime to load model: 10.28 seconds\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:34.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m457\u001b[0m - \u001b[1mEncoded text: やったー！今日のランチはカレーライスだ！いっぱい食べるぞー！\u001b[0m\n",
      "  3%|█▏                                      | 123/4276 [00:24<13:33,  5.10it/s]\n",
      "\u001b[32m2025-11-10 23:27:58.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m482\u001b[0m - \u001b[1mCompilation time: 24.76 seconds\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:58.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m491\u001b[0m - \u001b[1mGenerated 125 tokens in 24.76 seconds, 5.05 tokens/sec\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:58.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m494\u001b[0m - \u001b[1mBandwidth achieved: 4.34 GB/s\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:58.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m497\u001b[0m - \u001b[1mGPU Memory used: 3.19 GB\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:58.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m692\u001b[0m - \u001b[1mSampled text: やったー！今日のランチはカレーライスだ！いっぱい食べるぞー！\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:58.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m697\u001b[0m - \u001b[1mSaved codes to temp/codes_0.npy\u001b[0m\n",
      "\u001b[32m2025-11-10 23:27:58.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m698\u001b[0m - \u001b[1mNext sample\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python /nfs1/s1f102201582/projects/fish-speech/fish_speech/models/text2semantic/inference.py \\\n",
    "    --text \"やったー！今日のランチはカレーライスだ！いっぱい食べるぞー！\" \\\n",
    "    --prompt-text \"元気に\" \\\n",
    "    --prompt-tokens \"fake.npy\"  \\\n",
    "    --compile \\\n",
    "    --checkpoint-path \"/nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "494fe9d0-b9aa-4b53-ad78-3414e933ee60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs1/s1f102201582/anaconda3/envs/fish/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "\u001b[32m2025-11-10 23:28:50.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mLoaded model: <All keys matched successfully>\u001b[0m\n",
      "\u001b[32m2025-11-10 23:28:50.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mProcessing precomputed indices from temp/codes_0.npy\u001b[0m\n",
      "\u001b[32m2025-11-10 23:28:51.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mGenerated audio of shape torch.Size([1, 1, 253952]), equivalent to 5.76 seconds from 124 features, features/second: 21.53\u001b[0m\n",
      "\u001b[32m2025-11-10 23:28:51.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mSaved audio to fake.wav\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python /nfs1/s1f102201582/projects/fish-speech/fish_speech/models/dac/inference.py \\\n",
    "    -i \"temp/codes_0.npy\" \\\n",
    "    --checkpoint-path \"/nfs1/s1f102201582/projects/fish-speech/checkpoints/openaudio-s1-mini/codec.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899a1a8-6022-4b7a-a11c-0e28d54b1347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
