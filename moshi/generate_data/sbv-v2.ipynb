{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276dc24c-a03a-4f02-923b-4963f2307572",
   "metadata": {},
   "source": [
    "# FT用データ生成スクリプト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "683a8852-50c8-4ba2-a611-589ebd76bc9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !conda install -y -c conda-forge kalpy \\\n",
    "# kaldi \\\n",
    "# pynini\n",
    "\n",
    "# # パッケージインストール\n",
    "# !pip install -r requirements.sbv.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a816459b-765b-42cb-a7d1-10a1211a3fa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69a5268f-a2e3-48e3-ab2a-54e9e5e298e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local version of model already exists (/users/s1f102201582/Documents/MFA/pretrained_models/dictionary/japanese_mfa.dict). Use the --ignore_cache flag to force redownloading.\n",
      "Local version of model already exists (/users/s1f102201582/Documents/MFA/pretrained_models/acoustic/japanese_mfa.zip). Use the --ignore_cache flag to force redownloading.\n"
     ]
    }
   ],
   "source": [
    "# # mfa\n",
    "# # 日本語辞書のダウンロード\n",
    "# !mfa model download dictionary japanese_mfa\n",
    "\n",
    "# # 日本語音響モデルのダウンロード\n",
    "# !mfa model download acoustic japanese_mfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e391c-3079-4a7f-8eeb-e122e07f6707",
   "metadata": {},
   "source": [
    "## テキスト対話データ生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1666ef7b-1012-47c3-9229-6082e2d04f8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs1/s1f102201582/anaconda3/envs/mfa_unit/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "import ast\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# .envファイル読み込み\n",
    "load_dotenv(\"/users/s1f102201582/projects/mhcc-moshi/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473c6dfd-9d4d-4bac-904a-df31dcd1a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "from os.path import join, expanduser\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "BASE_URL = \"https://api.openai.iniad.org/api/v1\"\n",
    "MODEL='gemini-2.5-flash'\n",
    "TEMPERATURE = 1.0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "# 生成する音声のサンプリングレート\n",
    "setting_sr = 16000\n",
    "\n",
    "#対話音声データの個数を指定\n",
    "gen_dial_num = 863\n",
    "\n",
    "# すでに作成した対話データを削除するかどうか\n",
    "IS_REMOVE_EXIST_FILE = False\n",
    "\n",
    "version = \"v3\"\n",
    "\n",
    "# ftに使うjsonとaudioの出力フォルダパス\n",
    "home_dir = expanduser(\"~\")\n",
    "json_dir_path = join(home_dir, \"projects/mhcc-moshi/moshi/data\", version, \"data_stereo\")\n",
    "audio_dir_path = join(home_dir, \"projects/mhcc-moshi/moshi/data\", version, \"data_stereo\")\n",
    "\n",
    "# mfa関連のパス\n",
    "model_dir = join(home_dir, \"Documents/MFA/pretrained_models/acoustic/japanese_mfa.zip\")\n",
    "mfa_input_dir = join(home_dir, \"projects/mhcc-moshi/moshi/data\", version, \"mfa_input\")\n",
    "mfa_output_dir = join(home_dir, \"projects/mhcc-moshi/moshi/data\", version, \"mfa_output\")\n",
    "\n",
    "\n",
    "#RAGで読み取るPDFのパス\n",
    "rag_pdf_dir = join(home_dir, \"projects/mhcc-moshi/mental_docs/\")\n",
    "\n",
    "# SBVの音声合成モデルのパス\n",
    "assets_root = join(home_dir, \"projects/mhcc-moshi/moshi/model_assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf8bffea-83d1-44b8-9e3d-0bd71321ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_paths = [\n",
    "    json_dir_path,\n",
    "    audio_dir_path,\n",
    "    mfa_input_dir,\n",
    "    mfa_output_dir,\n",
    "]\n",
    "\n",
    "for p in base_paths:\n",
    "    if not os.path.isdir(p):\n",
    "        os.makedirs(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc14b9b-7981-4975-a07b-16114cf7b3af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model定義\n",
    "model = ChatGoogleGenerativeAI(\n",
    "                 model=MODEL,\n",
    "                 temperature=TEMPERATURE)\n",
    "\n",
    "# 埋め込みモデル定義\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=BASE_URL,\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# データベース定義\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"collection\",\n",
    "    embedding_function=embeddings,\n",
    "    # persist_directory = \"/path/to/db_file\" # if necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a271ae-62d4-44d1-b10d-8a8e38796ad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/3 [00:00<?, ?it/s]Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "    rag_pdf_dir,\n",
    "    glob=\"*.pdf\",\n",
    "    show_progress=True,\n",
    "    loader_cls=PDFMinerLoader,\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "669019e5-a634-40d8-b0f8-31abe0fb4877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Debug\n",
    "# for doc in docs:\n",
    "#     print(\"-------------------------------------------------\")\n",
    "#     print(doc.metadata)\n",
    "#     print(len(doc.page_content))\n",
    "#     print(doc.page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff84496-9aa7-45d9-89d3-3b8cde22fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#読み込んだ文章データをオーバーラップ200文字で1000文字づつ分割\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True, # 分割前の文章のインデックスを追跡\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# データベースにデータを追加\n",
    "document_ids = vector_store.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "757916d2-87c6-4033-8a23-02f0eaa6b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query, k=2)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d58e3775-2aad-4363-b893-f7d74f62d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Dialogue(BaseModel):\n",
    "    \"\"\"対話データを構成する対話クラス\"\"\"\n",
    "    speaker: Literal[\"A\", \"B\"] = Field(..., description=\"話者。Aはカウンセラー、Bはクライエントを表す。\")\n",
    "    text: str = Field(..., description=\"話者が話した内容。\")\n",
    "\n",
    "class Dialogues(BaseModel):\n",
    "    \"\"\"カウンセリングを目的としたカウンセリング対話データ\"\"\"\n",
    "    dialogues: list[Dialogue] = Field(..., description=\"対話データを構成する対話クラスのリスト。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c850f79b-c9ac-4722-b165-481de934e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model, \n",
    "    tools=[],\n",
    "    middleware=[prompt_with_context],\n",
    "    response_format=ToolStrategy(\n",
    "        Dialogues,\n",
    "        handle_errors=\"フォーマットに合うように、もう一度対話データを生成してください。\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "261f3908-86f7-4ddb-9565-c7b56e25632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#promptを作成\n",
    "import random\n",
    "\n",
    "prompt_template = \"\"\"あなたは、音声合成（TTS）用の対話シナリオを作成するプロのライターです。\n",
    "以下の【要件】、【設定】、そして【指示】に従い、専門のカウンセラー (C) と クライアント (CL) のテキスト対話データを生成してください。\n",
    "\n",
    "### 【目的】\n",
    "生成するテキストは、音声合成エンジンで読み上げることを前提としています。そのため、台本のような流暢な書き言葉ではなく、人間同士が実際に会話する際の「話し言葉」を忠実に再現することを最優先とします。\n",
    "\n",
    "### 【最重要要件：自然な会話の再現】\n",
    "以下の要素を必ずテキストに含めてください。\n",
    "\n",
    "1.  **極端に短い発話（ターン）の継続（最重要）:**\n",
    "    * 一度の発話（セリフ）は、意図的に非常に短く区切ってください。\n",
    "    * 人間同士の会話のように、短い言葉のキャッチボールが続くように構成してください。長い独白（モノローグ）や、一つの発話に多くの情報を詰め込むのは絶対に避けてください。\n",
    "\n",
    "2.  **発話の途中の相槌（インターラプト）:**\n",
    "    * これが最も重要です。**クライアント(CL)の発話**が少しでも長くなりそうな場合（例：10～15文字以上）、その**発話の途中に** **カウンセラー(C)**が「はい」「ええ」「うんうん」「なるほど」といった短い相槌を**独立した発話として**挟んでください。\n",
    "    * クライアントは、その相槌を受けて話を続けるという、現実の会話で頻繁に起こる言葉の重なりや短いキャッチボールを忠実に再現してください。\n",
    "\n",
    "3.  **クライアント(CL)の話し言葉の要素:**\n",
    "    * 「よどみ（フィラー）」や「言い直し・ためらい」は、【指示】5で指定された内容に従って調整してください。\n",
    "\n",
    "### 【設定】\n",
    "* **話者:**\n",
    "    * **C (カウンセラー):** 専門家。穏やかで、共感的・受容的なトーン。常に短い相槌を挟む。\n",
    "    * **CL (クライアント):** 悩みを抱えている。\n",
    "\n",
    "### 【指示】\n",
    "\n",
    "1.  **指定された設定:**\n",
    "    以下の指定された設定に基づいてシナリオを生成してください。\n",
    "    * **クライアントのペルソナ:** {selected_persona}\n",
    "    * **トピック:** {selected_topic}\n",
    "    * **状況:** {selected_situation}\n",
    "    * **クライアントの話し方:** {selected_style_name}\n",
    "\n",
    "2.  **会話の開始ルール（名前の扱い）:**\n",
    "    * シナリオは、必ず C (カウンセラー) がクライアントに呼びかける発話から始めてください。\n",
    "    * 指定された【状況】が『初回』の場合: カウンセラーがクライアントの**名前（呼び方）を尋ねる発話**（例：「はじめまして。担当します〇〇です。まず、お名前（呼び方）を伺ってもよろしいですか？」）から始めてください。\n",
    "    * 指定された【状況】が『初回』以外の場合: すでに名前は知っている設定とし、**名前を呼ばずに**（例：「こんにちは。その後の調子はいかがですか？」）始めてください。`〇〇さん`のようなプレースホルダーは**絶対に使用しないでください**。\n",
    "\n",
    "3.  **会話の長さ:**\n",
    "    * シナリオは、会話が**1分30秒〜2分程度**（テキスト量で約**8〜12往復程度**）続くようにしてください。\n",
    "\n",
    "4.  **要件の遵守:**\n",
    "    * 【最重要要件：自然な会話の再現】、特に「**1. 極端に短い発話**」と「**2. 発話の途中の相槌**」を厳密に守ってください。\n",
    "\n",
    "5.  **クライアントの話し方の調整（指示）:**\n",
    "    * 以下の指示に従って、**クライアント(CL)の発話**における話し方を調整してください。\n",
    "    * **指示:** {selected_style_instruction}\n",
    "\n",
    "6.  **ペルソナとトピックの統合（【ペルソナ】【トピック】変数に基づく）:**\n",
    "    * 指定された【クライアントのペルソナ】（年齢・職業/立場）と【トピック】（悩み）を組み合わせて、クライアント(CL)の**具体的な会話内容や背景**を構築してください。\n",
    "    * 例えば、ペルソナが「20代・会社員」でトピックが「職場の人間関係」なら、同僚や上司との関係に悩む若手社員の会話にしてください。\n",
    "    * **重要:** クライアント自身が「私は〇〇です」のように、自分のペルソナを台詞で**明言しない**ようにしてください。あくまで会話の内容から**推測できる**ようにしてください。\n",
    "\"\"\"\n",
    "\n",
    "# トピックリスト（悩みの種類）(全40項目)\n",
    "topic_list = [\n",
    "  # 仕事・キャリア関連\n",
    "  \"仕事のプレッシャーや過労、バーンアウト\",\n",
    "  \"職場の人間関係（上司、同僚、部下）\",\n",
    "  \"キャリアプランの悩み、キャリアチェンジの不安\",\n",
    "  \"転職・就職活動のストレス\",\n",
    "  \"ハラスメント（パワハラ、モラハラなど）の影響\",\n",
    "  \"仕事へのモチベーション低下、やる気が出ない\",\n",
    "\n",
    "  # 自己認識・感情関連\n",
    "  \"自己肯定感の低さ、自分を責めてしまう\",\n",
    "  \"完璧主義、失敗への過度な恐れ\",\n",
    "  \"他人の評価が過度に気になる、承認欲求\",\n",
    "  \"劣等感（他人との比較）\",\n",
    "  \"自分のやりたいことが分からない、アイデンティティの悩み\",\n",
    "  \"感情のコントロールが難しい（怒り、イライラ、悲しみ）\",\n",
    "  \"ネガティブ思考の癖、反芻思考（同じことをぐるぐる考えてしまう）\",\n",
    "  \"焦燥感、何かに追われている感覚\",\n",
    "  \"罪悪感（休むことへの罪悪感など）\",\n",
    "  \"虚無感、むなしさ、生きがいを感じられない\",\n",
    "  \"趣味や楽しみを感じられない（アンヘドニア）\",\n",
    "  \n",
    "  # 対人関係\n",
    "  \"家族関係（親子、夫婦、兄弟、親戚）\",\n",
    "  \"友人関係や恋愛関係の悩み\",\n",
    "  \"コミュニケーションへの苦手意識（雑談、会議での発言など）\",\n",
    "  \"人に頼ることができない、甘えられない\",\n",
    "  \"他人の期待に応えすぎようとしてしまう（ピープルプリーザー）\",\n",
    "  \"境界線（バウンダリー）の問題（NOと言えない）\",\n",
    "  \"孤独感、疎外感\",\n",
    "  \"HSP（繊細さ）に関する悩み\",\n",
    "  \n",
    "  # 生活・健康関連\n",
    "  \"将来への漠然とした不安\",\n",
    "  \"気分の落ち込み、無気力\",\n",
    "  \"睡眠に関する悩み（寝付けない、途中で起きる、過眠）\",\n",
    "  \"生活リズムの乱れ\",\n",
    "  \"体調不良（頭痛、倦怠感、腹痛など）と気分の関連\",\n",
    "  \"健康不安（病気への過度な心配）\",\n",
    "\n",
    "  # 習慣・行動関連\n",
    "  \"決断疲れ、何かを選ぶことができない\",\n",
    "  \"先延ばし癖、物事が始められない\",\n",
    "  \"SNS疲れ、デジタルデトックスの必要性\",\n",
    "\n",
    "  # 特定の出来事\n",
    "  \"ライフイベント（引っ越し、結婚、出産、育児、介護）に伴うストレス\",\n",
    "  \"特定の出来事による軽いトラウマやフラッシュバック\",\n",
    "  \"喪失体験（別れ、死別）からの回復（グリーフケア）\",\n",
    "  \"過去の選択への後悔\"\n",
    "]\n",
    "\n",
    "# 状況リスト（セッションの場面）\n",
    "situation_list = [\n",
    "  \"初回\",\n",
    "  \"初期\",\n",
    "  \"中期（深掘り）\",\n",
    "  \"中期（宿題の確認）\",\n",
    "  \"中期（感情の表出）\",\n",
    "  \"後期\",\n",
    "  \"終了（終結）\"\n",
    "]\n",
    "\n",
    "# クライアントのペルソナリスト（年齢と職業/立場のみ）\n",
    "persona_list = [\n",
    "    \"20代・会社員\",\n",
    "    \"30代・会社員\",\n",
    "    \"40代・会社員\",\n",
    "    \"50代・会社員\",\n",
    "    \"20代・大学生\",\n",
    "    \"20代・大学院生\",\n",
    "    \"30代・管理職\",\n",
    "    \"40代・主婦/主夫\",\n",
    "    \"30代・フリーランス\",\n",
    "    \"20代・アルバイト\",\n",
    "    \"40代・パートタイム\",\n",
    "    \"50代・自営業\",\n",
    "    \"20代・求職中\",\n",
    "    \"30代・育児休業中\"\n",
    "]\n",
    "\n",
    "speaking_style_data = {\n",
    "    \"よどみが多い（ためらいがち）\": \"CLの発話に、フィラー（「えーと」「あのー」「なんていうか」「その…」）や「言い直し・言いよどみ」を意図的に【多く】挿入してください。言葉がすぐに出てこない、ためらいながら話す様子を強く表現してください。\",\n",
    "    \"普通（自然な会話）\": \"【最重要要件】の指示通り、自然な会話の範囲で適度なフィラーと「言い直し」を挿入してください。\",\n",
    "    \"スムーズ（よどみ少なめ）\": \"CLの発話におけるフィラーや「言い直し・言いよどみ」を意図的に【最小限に】し、比較的流暢に話すようにしてください。（ただし、「1. 極端に短い発話」のルールは守り続けてください）\"\n",
    "}\n",
    "\n",
    "def gen_prompt_txt():\n",
    "    # ランダムに選択\n",
    "    selected_topic = random.choice(topic_list)\n",
    "    selected_situation = random.choice(situation_list)\n",
    "    selected_persona = random.choice(persona_list)\n",
    "    selected_style_name = random.choice(list(speaking_style_data.keys()))\n",
    "    selected_style_instruction = speaking_style_data[selected_style_name]\n",
    "\n",
    "    # print(\"選ばれたトピック:\", selected_topic)\n",
    "    # print(\"選ばれた状況:\", selected_situation)\n",
    "    # print(\"選ばれたペルソナ:\", selected_persona)\n",
    "    # print(\"選ばれたクライアントのスタイル:\", selected_style_name)\n",
    "\n",
    "    # 変数をプロンプトに埋め込む\n",
    "    final_prompt = prompt_template.format(\n",
    "        selected_persona=selected_persona,\n",
    "        selected_topic=selected_topic,\n",
    "        selected_situation=selected_situation,\n",
    "        selected_style_name=selected_style_name,\n",
    "        selected_style_instruction=selected_style_instruction\n",
    "    )\n",
    "    \n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d76aa80-c527-41a1-963a-5e60dc2307e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "\n",
    "max_retries = 5\n",
    "base_wait_time = 1 # minutes\n",
    "\n",
    "# テキスト対話生成関数\n",
    "def gen_txt_dialogue():\n",
    "    prompt = gen_prompt_txt()\n",
    "\n",
    "    # レート制限に引っかかることがあるため、例外処理\n",
    "    for i in range(1, max_retries+1):\n",
    "        try:\n",
    "            resp = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": prompt}]})\n",
    "            break\n",
    "        except ResourceExhausted as e:\n",
    "            if i < max_retries - 1:\n",
    "                wait_time = (base_wait_time ** i) * 60\n",
    "                time.sleep(wait_time)\n",
    "            # max_retries回失敗した場合はエラーを起こす\n",
    "            else:\n",
    "                raise e\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    dialogues_list = resp[\"structured_response\"].dialogues\n",
    "    return dialogues_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5813f865-a27c-473c-8acc-e8739dd606ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DEBUG\n",
    "# txt_dialogue = gen_txt_dialogue()\n",
    "# print(txt_dialogue)\n",
    "# lst_dialogue = txt_to_lst(txt_dialogue)\n",
    "# print(lst_dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd4da5-0ba6-4958-929d-c547e7c1f72c",
   "metadata": {},
   "source": [
    "## テキスト対話データを音声対話データに変換 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2caf61a3-b36a-48a1-8604-b8d720c5265d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m11-09 11:11:16\u001b[0m |\u001b[1m  INFO  \u001b[0m| bert_models.py:92 | Loaded the Languages.JP BERT model from ku-nlp/deberta-v2-large-japanese-char-wwm\n",
      "\u001b[32m11-09 11:11:17\u001b[0m |\u001b[1m  INFO  \u001b[0m| bert_models.py:154 | Loaded the Languages.JP BERT tokenizer from ku-nlp/deberta-v2-large-japanese-char-wwm\n",
      "jvnv-F2-jp/jvnv-F2_e166_s20000.safetensors\n",
      "jvnv-F2-jp/config.json\n",
      "jvnv-F2-jp/style_vectors.npy\n",
      "jvnv-M2-jp/jvnv-M2-jp_e159_s17000.safetensors\n",
      "jvnv-M2-jp/config.json\n",
      "jvnv-M2-jp/style_vectors.npy\n"
     ]
    }
   ],
   "source": [
    "from style_bert_vits2.nlp import bert_models\n",
    "from style_bert_vits2.constants import Languages\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "from style_bert_vits2.tts_model import TTSModel\n",
    "\n",
    "bert_models.load_model(Languages.JP, \"ku-nlp/deberta-v2-large-japanese-char-wwm\")\n",
    "bert_models.load_tokenizer(Languages.JP, \"ku-nlp/deberta-v2-large-japanese-char-wwm\")\n",
    "\n",
    "# # 子春音あみ\n",
    "# model_file = \"koharune-ami/koharune-ami.safetensors\"\n",
    "# config_file = \"koharune-ami/config.json\"\n",
    "# style_file = \"koharune-ami/style_vectors.npy\"\n",
    "# hf_repo = \"litagin/sbv2_koharune_ami\"\n",
    "\n",
    "# # あみたろ\n",
    "# model_file = \"amitaro/amitaro.safetensors\"\n",
    "# config_file = \"amitaro/config.json\"\n",
    "# style_file = \"amitaro/style_vectors.npy\"\n",
    "# hf_repo = \"litagin/sbv2_amitaro\"\n",
    "\n",
    "\n",
    "# デフォルトの女性2\n",
    "model_file = \"jvnv-F2-jp/jvnv-F2_e166_s20000.safetensors\"\n",
    "config_file = \"jvnv-F2-jp/config.json\"\n",
    "style_file = \"jvnv-F2-jp/style_vectors.npy\"\n",
    "hf_repo = \"litagin/style_bert_vits2_jvnv\"\n",
    "\n",
    "for file in [model_file, config_file, style_file]:\n",
    "    print(file)\n",
    "    hf_hub_download(hf_repo, file, local_dir=assets_root)\n",
    "\n",
    "A_model = TTSModel(\n",
    "    model_path=os.path.join(assets_root, model_file),\n",
    "    config_path=os.path.join(assets_root, config_file),\n",
    "    style_vec_path=os.path.join(assets_root, style_file),\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# デフォルトの男性2\n",
    "model_file = \"jvnv-M2-jp/jvnv-M2-jp_e159_s17000.safetensors\"\n",
    "config_file = \"jvnv-M2-jp/config.json\"\n",
    "style_file = \"jvnv-M2-jp/style_vectors.npy\"\n",
    "\n",
    "for file in [model_file, config_file, style_file]:\n",
    "    print(file)\n",
    "    hf_hub_download(hf_repo, file, local_dir=assets_root)\n",
    "\n",
    "B_model = TTSModel(\n",
    "    model_path=os.path.join(assets_root, model_file),\n",
    "    config_path=os.path.join(assets_root, config_file),\n",
    "    style_vec_path=os.path.join(assets_root, style_file),\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c975667-8174-4762-9db7-e8a602d2adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_audio_synth_prompt(text_dialogue_list):\n",
    "    resp = \"\"\n",
    "    resp_header =  \"\"\"あなたがこれから音声合成するテキストは以下の対話内容のワンフレーズです。\n",
    "この対話の文脈に合うように音声合成してください。\n",
    "\n",
    "<対話内容の全文>\"\"\"\n",
    "    resp += resp_header\n",
    "    for text_dial in text_dialogue_list:\n",
    "        resp += f\"\\n{text_dial.speaker}: {text_dial.text}\"\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "568879f5-82d9-488f-a281-d3ad816f248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def sbv_tts(text: str, speaker: Literal[\"A\", \"B\"], assist_text=None):\n",
    "    if speaker == \"A\":\n",
    "        sr, audio = A_model.infer(\n",
    "            text = text,\n",
    "            style='Happy',\n",
    "            style_weight=1,\n",
    "            split_interval = 0.3,\n",
    "            use_assist_text = True if assist_text is not None else None,\n",
    "            assist_text = assist_text\n",
    "        )\n",
    "    else:\n",
    "        sr, audio = B_model.infer(\n",
    "            text = text,\n",
    "            style='Sad',\n",
    "            style_weight=1,\n",
    "            split_interval = 0.3,\n",
    "            use_assist_text = True if assist_text is not None else None,\n",
    "            assist_text = assist_text\n",
    "        )\n",
    "    \n",
    "    return sr, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9df1471-cccf-40fb-bb30-23ba6fdeba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def gen_audio_dialogue(text_dialogue_list, prompt):\n",
    "    # 音声ファイルを順番に生成（ファイルは不要なのでwave配列で持つ）\n",
    "    wav_data = []\n",
    "    for dial in text_dialogue_list:\n",
    "        speaker = dial.speaker\n",
    "        sr, wav = sbv_tts(dial.text, speaker, prompt)\n",
    "\n",
    "        # サンプリングレートを変換\n",
    "        if sr != setting_sr:\n",
    "            # 16ビット整数のデータを、-1.0から1.0の範囲に収まる浮動小数点数に正規化\n",
    "            wav = wav.astype(np.float32) / 32768.0\n",
    "            wav = librosa.resample(wav, orig_sr=sr, target_sr=setting_sr)\n",
    "\n",
    "        # 0.3秒間の無音時間を追加\n",
    "        duration_sec = 0.3\n",
    "        num_silent_samples = int(setting_sr*duration_sec)\n",
    "        silence = np.zeros(num_silent_samples, dtype=wav.dtype)\n",
    "        wav_with_silence = np.concatenate((wav, silence))\n",
    "        wav_data.append(wav_with_silence)\n",
    "    \n",
    "    # 最終的な音声長を決定\n",
    "    max_len = sum([len(w) for w in wav_data])\n",
    "    \n",
    "    # ステレオ音声用（2チャンネル×最大長）の空配列をゼロ初期化で作成\n",
    "    stereo = np.zeros((2, max_len), dtype=np.float32)\n",
    "    \n",
    "    pos = 0\n",
    "    for i, wav in enumerate(wav_data):\n",
    "        ch = i%2  # 0:左(A), 1:右(B)\n",
    "        stereo[ch, pos:pos+len(wav)] += wav\n",
    "        pos += len(wav)\n",
    "    \n",
    "    # 転置(-1,2)する\n",
    "    stereo = stereo.T\n",
    "    return stereo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa95302-64ba-4175-ab42-4939b02ee1be",
   "metadata": {},
   "source": [
    "## mfa(montreal force alignment)による音声アラインメント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b201df8-c906-4cf7-ae5b-d94a7aa1e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def correct_json(full_text, align_json):\n",
    "    new_align_json = copy.deepcopy(align_json)\n",
    "    segments = new_align_json[\"tiers\"][\"words\"][\"entries\"]\n",
    "    checked_len = 0\n",
    "    prev_checked_len = 0\n",
    "    i = 0\n",
    "    while i < len(segments):\n",
    "        if re.search(f\"^<unk>|<sil>$\", segments[i][2]):\n",
    "            if i == 0:\n",
    "                if re.search(f\"^<unk>|<sil>$\", segments[i+1][2]):\n",
    "                    end_time = 0\n",
    "                    while re.search(f\"^<unk>|<sil>$\", segments[i+1][2]):\n",
    "                        end_time = segments[i+1][1]\n",
    "                        segments.pop(i+1)\n",
    "                    segments[i][1] = end_time\n",
    "                \n",
    "                m = re.search(f\"^(.+?){segments[i+1][2]}\", full_text[checked_len:])\n",
    "                match_text = m.groups()\n",
    "                segments[i][2] = match_text[0]\n",
    "            elif i == len(segments)-1:\n",
    "                m = re.search(f\"{segments[i-1][2]}(.+?)$\", full_text[checked_len:])\n",
    "                match_text = m.groups()\n",
    "                segments[i][2] = match_text[0]\n",
    "            else:\n",
    "                if re.search(f\"^<unk>|<sil>$\", segments[i+1][2]):\n",
    "                    end_time = 0\n",
    "                    while re.search(f\"^<unk>|<sil>$\", segments[i+1][2]):\n",
    "                        end_time = segments[i+1][1]\n",
    "                        segments.pop(i+1)\n",
    "                    segments[i][1] = end_time\n",
    "                m = re.search(f\"^{segments[i-1][2]}(.+?){segments[i+1][2]}\", full_text[prev_checked_len:])\n",
    "                match_text = m.groups()\n",
    "                segments[i][2] = match_text[0]\n",
    "        else:\n",
    "            if re.search(f\"^([。、,.!?！？…「」]){segments[i][2]}.*$\", full_text[checked_len:]):\n",
    "                m = re.search(f\"^([。、,.!?！？…「」]){segments[i][2]}.*$\", full_text[checked_len:])\n",
    "                match_punc = m.groups()\n",
    "                segments[i][2] = match_punc[0] + segments[i][2]\n",
    "            elif re.search(f\"^{segments[i][2]}([。、,.!?！？…「」]).*$\", full_text[checked_len:]):\n",
    "                m = re.search(f\"^{segments[i][2]}([。、,.!?！？…「」]).*$\", full_text[checked_len:])\n",
    "                match_punc = m.groups()\n",
    "                segments[i][2] = segments[i][2] + match_punc[0]\n",
    "                \n",
    "        prev_checked_len = checked_len\n",
    "        checked_len += len(segments[i][2])\n",
    "        i += 1\n",
    "    return new_align_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09aaef40-fca0-46e3-96b2-0c04a1ff984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, expanduser\n",
    "import subprocess\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "def alignment_channel(channel, target_dir_name):\n",
    "    input_dir_path = join(mfa_input_dir, target_dir_name)\n",
    "    output_dir_path = join(mfa_output_dir, target_dir_name)\n",
    "    os.makedirs(input_dir_path, exist_ok=True)\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "    subprocess.run([\n",
    "        \"mfa\",\n",
    "        \"align\",\n",
    "        input_dir_path,\n",
    "        \"japanese_mfa\",\n",
    "        model_dir,\n",
    "        output_dir_path,\n",
    "        \"--quiet\",\n",
    "        \"--overwrite\",\n",
    "        \"--clean\",\n",
    "        \"--final_clean\",\n",
    "        \"--output_format\", \"json\",\n",
    "        \"--beam\", \"1000\",\n",
    "        \"--retry_beam\", \"4000\",\n",
    "    ])      \n",
    "\n",
    "def parse_ft_json(json_data):\n",
    "    result = {\"alignments\": []}\n",
    "\n",
    "    segments = json_data[\"tiers\"][\"words\"][\"entries\"]\n",
    "    for segment in segments:\n",
    "        result[\"alignments\"].append([\n",
    "            segment[2],\n",
    "            [segment[0], segment[1]],\n",
    "            \"SPEAKER_MAIN\"\n",
    "        ])\n",
    "    result[\"alignments\"].sort(key=lambda x: x[1][0])\n",
    "    return result\n",
    "\n",
    "def alignment_audio_dialogue(text_dialogue_list, audio_path, idx):\n",
    "    json_list = []\n",
    "    audio, sr = sf.read(audio_path)\n",
    "    \n",
    "    result = \"\"\n",
    "    target_dir_name = str(idx)\n",
    "    target_dir = os.path.join(mfa_input_dir, target_dir_name)\n",
    "    if not os.path.isdir(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    target_text_file = os.path.join(target_dir, f\"{idx}.txt\")\n",
    "\n",
    "    oneline_text = \"\"\n",
    "    for dial in text_dialogue_list:\n",
    "        result += dial.text + \"\\n\"\n",
    "        oneline_text += dial.text\n",
    "    with open(target_text_file, \"w\") as f:\n",
    "        f.write(result)\n",
    "\n",
    "    wav_name = f\"{idx}.wav\"\n",
    "    src_wav_path = os.path.join(audio_dir_path, wav_name)\n",
    "    dist_wav_path = os.path.join(target_dir, wav_name)\n",
    "    shutil.copy(src_wav_path, dist_wav_path)\n",
    "\n",
    "    alignment_channel(audio, target_dir_name)\n",
    "    json_path = os.path.join(mfa_output_dir, target_dir_name, f\"{idx}.json\")\n",
    "    json_data = \"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    try:\n",
    "        correct_json_data = correct_json(oneline_text, json_data)\n",
    "        ft_json = parse_ft_json(correct_json_data)\n",
    "    except:\n",
    "        print(f\"jsonファイル {idx}.json の訂正に失敗しました。\")\n",
    "        ft_json= parse_ft_json(json_data)\n",
    "    \n",
    "    return ft_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b917fe-46ac-43a6-a5e7-c9903095616a",
   "metadata": {},
   "source": [
    "## フォルダ初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17a8d44f-adba-483e-9d5d-65dae116d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_file_name():\n",
    "    wav_file_pattern = r\"^(\\d+)\\.wav$\"\n",
    "    num = -1\n",
    "    for file in os.listdir(audio_dir_path):\n",
    "        if not os.path.exists(os.path.join(audio_dir_path, file)):\n",
    "            continue\n",
    "        if not re.match(wav_file_pattern, file):\n",
    "            continue\n",
    "    \n",
    "        match_obj = re.match(wav_file_pattern, file)\n",
    "        get_number = int(match_obj.groups()[0])\n",
    "    \n",
    "        if num < get_number:\n",
    "            num = get_number\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4895d7b4-d9e6-43c3-b4b6-d21b0d4eac50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import shutil\n",
    "\n",
    "def delete_files(dir_path):\n",
    "    shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "if IS_REMOVE_EXIST_FILE:\n",
    "    file_name_num = -1\n",
    "    for dir_path in base_paths:\n",
    "        delete_files(dir_path)\n",
    "else:\n",
    "    file_name_num = get_file_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535afd9-ad91-43af-a583-32cf4525864e",
   "metadata": {},
   "source": [
    "## メイン処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92baf05a-d14e-4a4b-8f3d-98bdc5043914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m11-09 11:11:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はじめまして。担当しますカウンセラーです。まず、お名前（呼び方）を伺ってもよろしいですか？\n",
      "\u001b[32m11-09 11:11:34\u001b[0m |\u001b[1m  INFO  \u001b[0m| infer.py:24 | Using JP-Extra model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs1/s1f102201582/anaconda3/envs/mfa_unit/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m11-09 11:11:39\u001b[0m |\u001b[1m  INFO  \u001b[0m| safetensors.py:50 | Loaded '/users/s1f102201582/projects/mhcc-moshi/moshi/model_assets/jvnv-F2-jp/jvnv-F2_e166_s20000.safetensors' (iteration 166)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs1/s1f102201582/anaconda3/envs/mfa_unit/lib/python3.12/site-packages/pyopenjtalk/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m11-09 11:11:48\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい、〇〇と申します。今日は、決めることが多くて、すごく疲れてしまっていて…\n",
      "\u001b[32m11-09 11:11:51\u001b[0m |\u001b[1m  INFO  \u001b[0m| infer.py:24 | Using JP-Extra model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs1/s1f102201582/anaconda3/envs/mfa_unit/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m11-09 11:11:54\u001b[0m |\u001b[1m  INFO  \u001b[0m| safetensors.py:50 | Loaded '/users/s1f102201582/projects/mhcc-moshi/moshi/model_assets/jvnv-M2-jp/jvnv-M2-jp_e159_s17000.safetensors' (iteration 159)\n",
      "\u001b[32m11-09 11:11:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい\n",
      "\u001b[32m11-09 11:11:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:55\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "その、特に育児休業中なんですけど、毎日小さなことでも、色々決めなきゃいけないことが多くて\n",
      "\u001b[32m11-09 11:11:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うんうん\n",
      "\u001b[32m11-09 11:11:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "それで、もう何を選んだらいいのか、わからなくなっちゃうんです\n",
      "\u001b[32m11-09 11:11:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど\n",
      "\u001b[32m11-09 11:11:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:56\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "例えば、子どもの服とか、離乳食のメニューとか\n",
      "\u001b[32m11-09 11:11:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい\n",
      "\u001b[32m11-09 11:11:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ほんの些細なことなんですけど、それでも、やっぱり、これでいいのかなって\n",
      "\u001b[32m11-09 11:11:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ\n",
      "\u001b[32m11-09 11:11:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ずっと考えてしまって、なかなか前に進めなくて\n",
      "\u001b[32m11-09 11:11:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:57\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "お気持ち、よくわかります\n",
      "\u001b[32m11-09 11:11:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい。それで、もう、何に対しても、『自分で決める』っていうのが、すごく億劫で\n",
      "\u001b[32m11-09 11:11:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うんうん\n",
      "\u001b[32m11-09 11:11:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "疲れてしまって、どうしたらいいのか、相談したくて\n",
      "\u001b[32m11-09 11:11:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:58\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そうでしたか\n",
      "\u001b[32m11-09 11:11:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "本当は、もっと、こう、ぱっと決められたら楽なのにって\n",
      "\u001b[32m11-09 11:11:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "はい\n",
      "\u001b[32m11-09 11:11:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "そう思っているんですけど、なかなかできなくて\n",
      "\u001b[32m11-09 11:11:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "ええ\n",
      "\u001b[32m11-09 11:11:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:11:59\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "もう、スーパーで買うもの一つでも、悩んでしまうくらいで\n",
      "\u001b[32m11-09 11:12:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:12:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "なるほど\n",
      "\u001b[32m11-09 11:12:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:12:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "本当に、些細なことなんですけど、それが、積み重なってしまって\n",
      "\u001b[32m11-09 11:12:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:12:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "うんうん\n",
      "\u001b[32m11-09 11:12:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:12:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "毎日が、すごく、しんどいなって\n",
      "\u001b[32m11-09 11:12:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n",
      "\u001b[32m11-09 11:12:00\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:259 | Start generating audio data from text:\n",
      "よくお話しくださいました\n",
      "\u001b[32m11-09 11:12:01\u001b[0m |\u001b[1m  INFO  \u001b[0m| tts_model.py:324 | Audio data generated successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Setting up corpus information\u001b[33m...\u001b[0m                                      \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Loading corpus from source files\u001b[33m...\u001b[0m                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Found \u001b[1;36m1\u001b[0m speaker across \u001b[1;36m1\u001b[0m file, average number of utterances per       \n",
      "\u001b[2;36m \u001b[0m         speaker: \u001b[1;36m1.0\u001b[0m                                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Initializing multiprocessing jobs\u001b[33m...\u001b[0m                                  \n",
      "\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Number of jobs was specified as \u001b[1;36m3\u001b[0m, but due to only having \u001b[1;36m1\u001b[0m speakers, \n",
      "\u001b[2;36m \u001b[0m         MFA will only use \u001b[1;36m1\u001b[0m jobs. Use the --single_speaker flag if you would  \n",
      "\u001b[2;36m \u001b[0m         like to split utterances across jobs regardless of their speaker.     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Normalizing text\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating MFCCs\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Calculating CMVN\u001b[33m...\u001b[0m                                                   \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating final features\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Creating corpus split\u001b[33m...\u001b[0m                                              \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Compiling training graphs\u001b[33m...\u001b[0m                                          \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Performing first-pass alignment\u001b[33m...\u001b[0m                                    \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Generating alignments\u001b[33m...\u001b[0m                                              \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Collecting phone and word alignments from alignment lattices\u001b[33m...\u001b[0m       \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Analyzing alignment quality\u001b[33m...\u001b[0m                                        \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Exporting alignment TextGrids to                                      \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/projects/mhcc-moshi/moshi/data/v2/mfa_output/\u001b[0m\u001b[95m136..\u001b[0m\n",
      "\u001b[2;36m \u001b[0m         \u001b[95m.\u001b[0m                                                                     \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Finished exporting TextGrids to                                       \n",
      "\u001b[2;36m \u001b[0m         \u001b[35m/users/s1f102201582/projects/mhcc-moshi/moshi/data/v2/mfa_output/\u001b[0m\u001b[95m136\u001b[0m! \n",
      "\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Done! Everything took \u001b[1;36m171.288\u001b[0m seconds                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 4.21 s, total: 1min 10s\n",
      "Wall time: 3min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import soundfile as sf\n",
    "import json\n",
    "\n",
    "for i in range(file_name_num+1, gen_dial_num+file_name_num+1):\n",
    "\n",
    "    # テキスト生成\n",
    "    txt_dialogue_list = gen_txt_dialogue()\n",
    "\n",
    "    # 音声合成のためのプロンプト生成\n",
    "    audio_synth_prompt = build_audio_synth_prompt(txt_dialogue_list)\n",
    "\n",
    "    # 対話テキストを音声合成\n",
    "    stereo = gen_audio_dialogue(txt_dialogue_list, audio_synth_prompt)\n",
    "    \n",
    "    wav_name = f\"{i}.wav\"\n",
    "    audio_file_path = os.path.join(audio_dir_path, wav_name)\n",
    "\n",
    "    # wavファイル出力\n",
    "    sf.write(audio_file_path, stereo, setting_sr)\n",
    "\n",
    "    # 音声アラインメント\n",
    "    json_data = alignment_audio_dialogue(txt_dialogue_list, audio_file_path, i)\n",
    "\n",
    "    json_name = f\"{i}.json\"\n",
    "    json_file_path = os.path.join(json_dir_path, json_name)\n",
    "    \n",
    "    # JSON出力\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c4430-8682-4ac1-9921-1d4457ee0572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
